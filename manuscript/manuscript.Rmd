---
title             : "Uncertain Science is Bad Science: Lay Perceptions of Crowd-Scientific Findings"
shorttitle        : "The Sway and Credibility of Multi-Analyst Studies"

author: 
  - name          : "Shilaan Alzahawi"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "655 Knight Way, Stanford, CA 94305"
    email         : "shilaan@stanford.edu"
  - name          : "Beno√Æt Monin"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Stanford University, Graduate School of Business"

abstract: |
  A recent movement towards crowd science has emerged in the behavioral sciences. Crowd science aims to increase the rigor, reliability, and credibility of scientific research. Does it meet its promises in reality? We report the results of an experiment in which we explore whether scientific findings emerging from a crowd of researchers (vs. a typical research collaboration) are more likely to sway the prior beliefs of research consumers, increase ratings of credibility and confidence, and decrease ratings of bias and error. We focus on crowdsourced data analysis, also known as the 'many analysts' or 'multi-analyst' approach: giving the same dataset to different teams of scientists, who independently analyze it to answer the same research question.  
  In line with our hypotheses, we find that lay consumers of multi-analyst studies with inconsistent results (compared to single-analyst studies) are less likely to be swayed, find the results less credible, have less confidence in the average effect size estimate, and believe the results are more likely to stem from bias and error. Contrary to our hypotheses, we do not find that multi-analyst studies with consistent results (compared to single-analyst studies) increase the sway and credibility of scientific research to lay consumer. 
  
keywords          : "Meta-science, Crowd science, Many analysts, Multi-analyst, Research credibility, Science skepticism"
wordcount         : "X"

bibliography      : ["r-references.bib", "library.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
# devtools::install_github("crsh/papaja@devel")
library(papaja)
library(raincloudplots)
library(ggplot2)
library(broom)
library(purrr)
library(glue)
library(readr)
library(dplyr)
library(rmarkdown)
library(here)
r_refs("r-references.bib")
```

The credibility of scientific research is in doubt, among lay consumer and scientist alike. What can be done about this "crisis of confidence" [@pashler2012: p. 528]? Several tools have been proposed to improve the reliability of scientific research and combat the crisis of confidence. One such tool is the crowd science approach, which leverages a large number of individuals or teams at specific stages of the research process [@uhlmann2019].   
Crowd science aims to improve the rigor, reliability, and credibility of scientific research. Does it meet its promises in reality? Here, we focus on crowdsourced data analysis, also known as the 'many analysts' or 'multi-analyst' approach: giving the same dataset to different teams of scientists, who independently analyze it to answer the same research question. We explore whether multi-analyst studies increase the sway and credibility of scientific research to lay consumers. That is, are scientific findings emerging from a crowd of researchers (vs. a typical science collaboration) more likely to sway the prior beliefs of research consumers, increase ratings of credibility and confidence, and decrease ratings of bias and error?     

# Methods
The preregistration of our experiment can be found in the Open Science Foundation (OSF) Registries at https://osf.io/rpu98. All data and code needed to reproduce this article can be found on GitHub at https://github.com/shilaan/many-analysts and the OSF at https://osf.io/vedb4/. We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.    

## Participants
We sampled participants from the Prolific participant pool. We included participants from the United States or the United Kingdom who were at least 18 years old and spoke English as a first language. We excluded participants who failed the attention check or attempted to take the survey more than once. Our target sample size was 1500 participants after exclusions. We aimed to keep sampling until reaching the desired number of valid participants or until 2 weeks passed (whichever came first). We expected relatively small effect sizes and decided on an upper limit on the number of participants based on monetary constraints.   

## Procedure and materials
All survey materials can be found on the OSF at https://osf.io/md9z5/. After a brief introduction to a research question ("Do religious people report higher well-being?"), participants were asked to report their prior beliefs ("How likely do you think it is that people who are more religious generally report higher well-being?") on a slider from 0% (not likely at all) to 100% (extremely likely).  
After reporting their prior beliefs, participants were randomly allocated to one of three experimental conditions in which they learned about the approach and findings of a scientific study. In the single-analyst condition, a single team of six researchers reports a 5% increase in well-being among religious people; in the multi-consistent condition, six independent researchers report six  consistent estimates (2%, 4%, 5%, 5%, and 6%, respectively) that average to 5%; and in the multi-inconsistent condition, six independent researchers report six inconsistent estimates (-6%, -2%, 5%, 5%, 12%, and 16%, respectively) that average to 5%.     
Afterwards, participants rated (1) their final beliefs about the research question, (2) the credibility of the results, (3) their confidence in the effect size estimate, and how likely it is that the estimate was influenced by (4) bias, (5) error, and (6) degrees of freedom. All questions were answered on a slider from 0% (not likely/credible/confident at all) to 100% (extremely likely/credible/confident). 

## Data analysis
For all six measures, we run linear regression models with condition as the independent variable (with the single-analyst condition as the reference category) and prior beliefs as a covariate. All hypotheses, statistical models, and code were preregistered at https://osf.io/rpu98. An overview of our preregistered, directional hypotheses can be found in Table 1. We did not preregister any hypotheses for the last measure; the findings concerning the impact of experimental condition on ratings of degrees of freedom are exploratory, and should be treated as such. Because we test five separate hypotheses using two comparisons each (one comparison of the single-analyst vs. the multi-consistent condition, and one comparison of the single-analyst vs. the multi-inconsistent condition), we use the Bonferroni method to correct for multiple comparisons. Thus, our preregistered threshold for statistical significance is $\displaystyle \alpha = \frac{.05}{2} = .025$.  

# Results

```{r data-cleaning, include = FALSE}
df <- read.csv(paste0(here(), "/data/data.csv"))
df <- df[3:nrow(df),]
n_recorded <- nrow(df)

df <- df %>% #take out participants who attempted to take the survey more than once
  filter(!PROLIFIC_PID %in% df$PROLIFIC_PID[duplicated(df$PROLIFIC_PID)])
n_more_than_once <- n_recorded - nrow(df)

df <- df %>% #take out participants who were screened out or did not consent
  filter(Consent == "I agree to participate in this research project")
n_screened_out <- n_recorded - n_more_than_once - nrow(df)

df <- df %>% #take out participants who failed or did not complete the attention check
  filter(Condition == Attention_check & Attention_check != "") 
n_failed_attention <- n_recorded - n_more_than_once - n_screened_out - nrow(df)

df <- df %>% 
  mutate(
    ID = 1:nrow(df),
    Condition = factor(Condition, #set reference category
                       levels = c("Single",           
                                  "Multi-consistent", 
                                  "Multi-inconsistent"))
  ) %>% 
  rename(
    Prior_beliefs = Prior.beliefs_1,
    Final_beliefs = Final.beliefs_1,
    Credibility = Credibility_1,
    Confidence = Confidence_1,
    Bias = Sources.Variability_1,
    Error = Sources.Variability_4,
    Discretion = Sources.Variability_5
  ) %>% 
  select(ID, Condition, Attention_check, Prior_beliefs:Discretion)

df[,4:ncol(df)] <- sapply(df[,4:ncol(df)], as.numeric) #recode variables to numeric
```

We recorded `r n_recorded` responses in Qualtrics. We excluded `r n_more_than_once` observations from participants who attempted to take the survey more than once, `r n_screened_out` participants who were screened out prior to starting the survey or did not consent, and `r n_failed_attention` participants who failed the attention check. This left us with a total sample of `r nrow(df)` participants (500 per condition).  
```{r, include = FALSE}
results <- function(Predictor, Multi_Condition = "Consistent") {
  linear_mod <- lm(
    pull(df, {{ Predictor }}) ~ Condition + Prior_beliefs, data = df)
  
  apa_results <- apa_print(linear_mod)
  
  ifelse(Multi_Condition == "Consistent",
         paste(apa_results$estimate$ConditionMulti_consistent,
               apa_results$statistic$ConditionMulti_consistent,
               sep = ", "),
         paste(apa_results$estimate$ConditionMulti_inconsistent,
               apa_results$statistic$ConditionMulti_inconsistent,
               sep = ", ")
         )
}
```
Our main findings are displayed in Figure 1. Controlling for prior beliefs and comparing to the single-analyst condition, we found that (1) reported final beliefs were significantly lower in both the multi-consistent condition, `r results(Final_beliefs)`, and the multi-inconsistent condition, `r results(Final_beliefs, "Inconsistent")`; (2) ratings of credibility were significantly lower in the multi-inconsistent condition, `r results(Credibility, "Inconsistent")`, while they were not significantly different in the multi-consistent condition, `r results(Credibility)`; (3) confidence in the effect size estimate was significantly lower in the multi-inconsistent condition, `r results(Confidence, "Inconsistent")`, while it was not significantly different in the multi-consistent condition, `r results(Confidence)`; (4) ratings of bias were significantly greater in the multi-inconsistent condition, `r results(Bias, "Inconsistent")`, while they were not significantly different in the multi-consistent condition, `r results(Bias)`; and (5) ratings of error were significantly greater in both the multi-consistent condition, `r results(Error)`, and the multi-inconsistent condition, `r results(Error, "Inconsistent")`. For our exploratory measure of discretion, we found that ratings of degrees of freedom were significantly greater in both the multi-consistent condition, `r results(Discretion)`, and the multi-inconsistent condition, `r results(Discretion, "Inconsistent")`.   
In sum, all preregistered hypotheses (see Table 1) related to the multi-consistent condition were not corroborated; all preregistered hypotheses related to the multi-inconsistent condition were corroborated. In line with our hypotheses, multi-analyst studies with inconsistent results decrease the sway and credibility of scientific research to lay consumers. Contrary to our hypotheses, multi-analyst studies with consistent result do not increase the sway and credibility of scientific research to lay consumers. Figure 2 further clarifies the sway of multi-analyst vs. single-analyst studies, by displaying the distribution of prior and final beliefs across the three conditions.    

# Discussion

From the proliferation of big team science and large-scale replication initiatives to preregistration and registered reports, several scientific fields have undergone significant reform with the well-intended goal of improving the rigor and reliability of scientific research. However, as with any real-world intervention, scientific reform can have unintended consequences. Here, we focused on the effects of crowdsourcing data analysis, and find that the multi-analyst approach may have an unintended consequence. While instituted with the goal of improving the credibility of scientific research, lay consumers appear to resist the inherent uncertainty that comes with inconsistent estimates. As a result, they are less likely to believe in the reported phenomenon, rate the scientific finding as less credible, have less confidence in the reported effect size estimate, and are more likely to think that the findings stem from bias and error. 

## Acknowledgements
This manuscript was created using `r cite_r("r-references.bib")`. We thank Nicole Clare Kolmstetter for valuable assistance in the data collection.  

## Data availability statement
The data that support the findings of this study are openly available on GitHub at https://github.com/shilaan/many-analysts and the OSF at https://osf.io/vedb4/.   

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage

# Tables and Figures

## Table 1
```{r, echo = FALSE}
table1 <- data.frame(
  Measure = c("1. Final beliefs",
              "2. Credibility", 
              "3. Confidence",
              "4. Bias",
              "5. Error"),
  Multi_Consistent = c("+", "+", "-", "-", "-"),
  Multi_Inconsistent = c("-", "-", "-", "+", "+"))
  
knitr::kable(table1,
             col.names = c("Measure", 
                           "Many-analyst: Consistent", 
                           "Many-analyst: Inconsistent"),
             align = "lcc",
             label = NA,
             caption = "Predicted direction of effects for all dependent variables, compared to the single-analyst condition and controlling for prior beliefs")
```
*Note*. Table 1 indicates the predicted direction of the effect for each of the five dependent variables, compared to the single-analyst condition and controlling for prior beliefs. For example, we hypothesized that, compared to a single-analyst study and controlling for prior beliefs,  ratings of credibility would be greater in the multi-analyst: consistent condition and lower in the multi-analyst: inconsistent condition.  

## Figure 1
*Ratings of Bias, Confidence, Credibility, Discretion, Error, and Final Beliefs*
\noindent
```{r fig-1, echo=FALSE}
tidy_df <- function(Predictor) {
  linear_mod <- lm(
    pull(df, {{ Predictor }}) ~ Condition + Prior_beliefs, data = df)
  
  tidy_lm <- tidy(linear_mod, conf.int = TRUE) %>% 
    mutate(Measure = glue("{Predictor}")) %>% 
    filter(term != "Prior_beliefs" & term != "(Intercept)")
  tidy_lm
}

Predictors <- list("Final_beliefs", 
                  "Credibility", 
                  "Confidence", 
                  "Bias", 
                  "Error", 
                  "Discretion")

tidy_results <- Predictors %>% 
  map(~ tidy_df(.x)) %>% 
  bind_rows() %>% 
  mutate(term = gsub("Condition", "", term))

Measure.label = c("Final beliefs", Predictors %>% unlist() %>% tail(5))
names(Measure.label) <- Predictors %>% unlist()

p1 <- ggplot(mapping = aes(x = term, y = estimate), 
             data = tidy_results) +
  geom_point(size = 2) +
  geom_errorbar(mapping = aes(ymin = conf.low, ymax = conf.high), 
                size = 0.8, width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray34", alpha = 0.7) +
  facet_wrap(~ Measure, 
             labeller = labeller(Measure = Measure.label)) + 
  theme_bw() + 
  theme(strip.background = element_rect(fill = "white"),
        legend.position = "") +
  labs(x = "Condition", y = "Estimate and 95% Confidence Interval") +
   scale_x_discrete(labels = c("Multi-\nconsistent", "Multi-\ninconsistent")) 

ggsave("Figure1.jpg", p1, 
       width = 7, height = 5, units = "in")

knitr::include_graphics("Figure1.jpg", dpi = 1200)
```
*Note.* Figure 1 displays coefficient estimates (and 95% confidence intervals) of bias, confidence, credibility, discretion, error, and final beliefs in the two multi-analyst conditions, compared to the single-analyst condition (and controlling for prior beliefs).  

\newpage

## Figure 2

*Individual data points, quartiles, and distributions of prior and final beliefs in the single-analyst, multi-consistent, and multi-inconsistent conditions*  
\noindent
```{r raincloud, echo=FALSE}
df_2x3 <- data_2x2(
  array_1 = df %>% filter(Condition == "Multi-inconsistent") %>% pull(Prior_beliefs),
  array_2 = df %>% filter(Condition == "Multi-inconsistent") %>% pull(Final_beliefs),
  array_3 = df %>% filter(Condition == "Multi-consistent") %>% pull(Prior_beliefs),
  array_4 = df %>% filter(Condition == "Multi-consistent") %>% pull(Final_beliefs),
  array_5 = df %>% filter(Condition == "Single") %>% pull(Prior_beliefs),
  array_6 = df %>% filter(Condition == "Single") %>% pull(Final_beliefs),
  labels = (c('Prior Beliefs','Final Beliefs')),
  jit_distance = .09,
  jit_seed = 321) 

colors <- rep(c("dodgerblue", "darkorange"), 3) #choose colors 

p2 <- raincloud_2x3_repmes(
  data = df_2x3,
  colors = colors,
  fills = colors,
  size = 1,
  alpha = .6,
  ort = "h") + #set to v for vertical plot
  
  scale_x_continuous(
    breaks = c(1,2,3), 
    limits = c(0.8, 4.3), 
    labels = rep("", 3)) +
  ylab("Rated Beliefs") +
  
  annotate(geom = "text", 
           label = "Single-Analyst", 
           x = 3.9, y = 13, hjust = 1) + 
  annotate(geom = "text", 
           label = "Multi-Analyst: Consistent", 
           x = 2.7, y = 11) + 
  annotate(geom = "text", 
           label = "Multi-Analyst: Inconsistent",
           x = 1.75, y = 12) + 
  annotate(geom = "text", 
           label = "Prior Beliefs", 
           x = 4.2, y = 37, size = 5, 
           color = "dodgerblue") + 
  annotate(geom = "text", 
           label = "vs.", 
           x = 4.2, y = 50, size = 5) + 
  annotate(geom = "text", 
           label = "Final Beliefs", 
           x = 4.2, y = 63, size = 5, 
           color = "darkorange") + 
  
  theme_classic() +
  theme(axis.ticks.y = element_blank(),
        axis.text = element_text(size = 9),
        axis.title.y = element_blank())

ggsave("Figure2.jpg", p2, 
       width = 7, height = 5, units = "in")

knitr::include_graphics("Figure2.jpg", dpi = 1200)
```
*Note. * Prior beliefs are displayed in blue; final beliefs are displayed in orange. The respective boxes display the lower quartiles, medians, and upper quartiles of prior and final beliefs by condition. 

