---
title             : "Lay Perceptions of Crowd-Scientific Findings: The Risks of Variability and Lack of Consensus"
shorttitle        : "Lay Perceptions of Multi-Analyst Studies"

author: 
  - name          : "Shilaan Alzahawi"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "655 Knight Way, Stanford, CA 94305"
    email         : "shilaan@stanford.edu"
  - name          : "Benoît Monin"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Stanford University, Graduate School of Business"

abstract: |
  A recent movement towards crowd science has emerged in the behavioral sciences. One of the aims of crowd science is to increase the credibility of scientific research. Does it meet this promise in reality? We run an experiment to study the effects of scientific findings emerging from a crowd of researchers (vs. a typical research collaboration) on lay consumers' posterior beliefs and ratings of credibility, confidence in an aggregate effect size estimate, bias, and error. We focus on crowdsourced data analysis, also known as the 'many analysts' or 'multi-analyst' approach: giving the same dataset to different teams of scientists, who independently analyze the data to answer a research question and/or estimate a parameter of interest. We compare the effects of providing a single, aggregate parameter estimate (the single-analyst condition) vs. multiple parameter estimates that (a) vary slightly and are all positive, leading to the same qualitative conclusion (the "multi-consistent" condition) or (b) vary widely and are of both signs, leading to differing qualitative conclusions (the "multi-inconsistent" condition). In line with our hypotheses, we find that lay consumers of multi-analyst studies with inconsistent results (compared to single-analyst studies) have lower posterior beliefs, find the results less credible, have less confidence in the average effect size estimate, and believe the results are more likely to stem from bias and error. Contrary to our hypotheses, we do not find that multi-analyst studies with consistent results (compared to single-analyst studies) increase the sway and credibility of scientific findings to lay consumers: instead, to our surprise, they lead to lower posterior beliefs and higher ratings of error. 
  
keywords          : "Crowd science, Many analysts, Multi-analyst, Variability, Research credibility"
wordcount         : "3,710"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
# devtools::install_github("crsh/papaja@devel")
library(papaja)
library(raincloudplots)
library(ggplot2)
library(broom)
library(purrr)
library(glue)
library(readr)
library(dplyr)
library(rmarkdown)
library(here)
library(icons)
library(rsvg)
r_refs("r-references.bib")
```

The credibility of scientific research is in doubt, among lay consumer [@hornsey2017] and scientist [@pashler2012] alike. Several tools have been proposed to combat this "crisis of confidence" (Ibid., p. 528). One such tool is the crowd science approach, which leverages a large number of individuals or teams at specific stages of the research process [@uhlmann2019].  
Crowd science is defined as "the organization of scientific research in open and collaborative projects"  [@franzoni2014, p. 1]. Crowd-scientific projects vary in their degree of selectivity: some are open to the general public (such projects are often referred to as "citizen science"), while others require the expertise of specialists [e.g., scientists\; @uhlmann2019, p. 714]. Here, we focus on crowdsourced data analysis, also known as the 'many analysts' or 'multi-analyst' approach: giving the same dataset to different teams of scientists, who independently analyze it to answer the same research question and/or estimate a parameter of interest.  
We explore the effects of scientific findings emerging from a crowd of researchers (vs. a typical research collaboration) on lay consumers' posterior beliefs, perceptions of credibility, confidence in an aggregate effect size estimate, and ratings of bias and error. We compare the effects of providing a single, aggregate parameter estimate (the single-analyst condition) vs. multiple parameter estimates that (a) vary slightly and are all positive, leading to the same qualitative conclusion (the "multi-consistent" condition) or (b) vary widely and are of both signs, leading to differing qualitative conclusions (the "multi-inconsistent" condition).    
According to previous work, crowdsourced data analysis should increase the credibility of scientific findings; Table 1 provides an overview of the proposed confidence- and credibility-related benefits of the multi-analyst approach. Does crowd science meet its promise -- to improve the credibility of scientific research -- in reality? Normatively, we expect laypeople who observe several scientists independently come to the same qualitative conclusions -- compared to a single research team that comes to a single, joint conclusion -- to be more swayed by the findings, as it seems to reflect consensus between various approaches. After all, it's a hallmark of rigorous research to triangulate various methodological approaches and provide converging evidence [@jick1979; @turner2017], and to provide a "sensitivity analysis" or "robustness checks" to examine whether and how the findings change as a result of alternative analytic specifications [@munoz2018; @steegen2016]. As argued in Aczel et al. [-@aczel2020, p. 562], "The main argument for the importance of performing robustness checks over reasonable variations in modelling choices is to increase confidence in the obtained results: ideally, results should be reasonably unaffected by a researcher's idiosyncratic choice (…) when reasonable alternatives exist." The same argument has been made for triangulation, which, according to Jick [-@jick1979, p. 608], "allows researchers to be more confident of their results."  
We similarly expect readers of multi-analyst reports that yield consistent findings to be more confident in the results. Although lay observers, in such cases, may be less certain that the precise aggregate parameter estimate is exactly correct, they should be more likely to positively update their beliefs about the reported phenomenon, and less likely to assume that the overall estimate stems from bias or error. Thus, when the results generated by independent analysts are largely consistent, we expect an increase in the sway of scientific findings. However, when laypeople observe several scientists independently come to differing qualitative conclusions, we expect the multi-analyst method to backfire; when results across many analysts vary widely and lack consensus in their qualitative conclusions (which, arguably, often reflects the reality of large-scale science collaborations), we expect a decrease in the sway of scientific findings.  
**[Insert Table 1 here]**  
Our pre-registered hypotheses (https://osf.io/rpu98) can be found in Table 2: we hypothesized that in the multi-consistent condition (compared to the single-analyst condition), lay consumers would have higher posterior beliefs, would find the results more credible, and would be less likely to believe the results stem from bias or error. For the multi-inconsistent condition, we hypothesized that lay consumers would have lower posterior beliefs, would find the results less credible, and would be more likely to believe the results stem from bias or error. In addition, we expected that the act of providing multiple (slightly to widely varying) parameter estimates would decrease confidence in the aggregate parameter estimate in both multi-analyst conditions.  
**[Insert Table 2 here]**  

<!-- "Following the logic of the wisdom of the crowds, in which aggregating estimates reduces individual level biases (Galton, 1907; Lorge, Fox, Davitz, & Brenner, 1958; Surowiecki, 2004), the central tendency of the effect size estimates calculated by many different analysts may provide a less subjective and error-prone estimate of the effect."  (Schweinsberg et al., 2021: p. 246-7) -->

# Methods
The preregistration of our experiment can be found in the Open Science Foundation (OSF) Registries at https://osf.io/rpu98. All data and code needed to reproduce this article can be found on GitHub at https://github.com/shilaan/many-analysts and the OSF at https://osf.io/vedb4/. We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.    

## Participants
We sampled participants from the Prolific participant pool. We included participants from the United States or the United Kingdom who were at least 18 years old and spoke English as a first language. We excluded participants who failed the attention check or attempted to take the survey more than once. Our target sample size was 1,500 participants after exclusions. We aimed to keep sampling until reaching the desired number of valid participants or until 2 weeks passed (whichever came first). We expected relatively small effect sizes and decided on an upper limit on the number of participants based on monetary constraints.   

## Procedure and materials
All survey materials can be found on the OSF at https://osf.io/md9z5/. After a brief introduction to a research question ("Do religious people report higher well-being?"), participants were asked to report their prior beliefs ("How likely do you think it is that people who are more religious generally report higher well-being?") on a slider from 0% (not likely at all) to 100% (extremely likely).  
After reporting their prior beliefs, participants were randomly allocated to one of three experimental conditions in which they learned about the approach and findings of a scientific study. In the single-analyst condition, a single team of six researchers reports a 5% increase in well-being among religious people; in the multi-consistent condition, six independent researchers report six  consistent estimates (2%, 4%, 5%, 5%, 6%, and 8%, respectively) that average to 5% (SD = 2); and in the multi-inconsistent condition, six independent researchers report six inconsistent estimates (-6%, -2%, 5%, 5%, 12%, and 16%, respectively) that average to 5% (SD = 8.25).     
Afterwards, participants rated (1) their posterior beliefs, (2) the credibility of the results, (3) their confidence in the effect size estimate, and how likely it is that the estimate was influenced by (4) bias, (5) error, and (6) experimenter degrees of freedom. All questions were answered on a slider from 0% (not likely/credible/confident at all) to 100% (extremely likely/credible/confident). 

## Data analysis
For all six measures, we run linear regression models with condition as the independent variable (with the single-analyst condition as the reference category) and prior beliefs as a covariate. All hypotheses, statistical models, and code were preregistered at https://osf.io/rpu98. An overview of our preregistered, directional hypotheses can be found in Table 2. We did not preregister any hypotheses for the last measure; the findings concerning the impact of experimental condition on ratings of experimenter degrees of freedom are exploratory, and should be treated as such. Because we test five separate hypotheses using two comparisons each (one comparison of the single-analyst vs. the multi-consistent condition, and one comparison of the single-analyst vs. the multi-inconsistent condition), we use the Bonferroni method to correct for multiple comparisons. Thus, our preregistered threshold for statistical significance is $\displaystyle \alpha = \frac{.05}{2} = .025$.  

# Results

```{r data-cleaning, include = FALSE}
df <- read.csv(paste0(here(), "/data/data.csv"))
df <- df[3:nrow(df),]
n_recorded <- nrow(df)

df <- df %>% #take out participants who attempted to take the survey more than once
  filter(!PROLIFIC_PID %in% df$PROLIFIC_PID[duplicated(df$PROLIFIC_PID)])
n_more_than_once <- n_recorded - nrow(df)

df <- df %>% #take out participants who were screened out or did not consent
  filter(Consent == "I agree to participate in this research project")
n_screened_out <- n_recorded - n_more_than_once - nrow(df)

df <- df %>% #take out participants who failed or did not complete the attention check
  filter(Condition == Attention_check & Attention_check != "") 
n_failed_attention <- n_recorded - n_more_than_once - n_screened_out - nrow(df)
ns_bycondition <- table(df$Condition)

df <- df %>% 
  mutate(
    ID = 1:nrow(df),
    Condition = factor(Condition, #set reference category
                       levels = c("Single",           
                                  "Multi-consistent", 
                                  "Multi-inconsistent"))
  ) %>% 
  rename(
    Prior_beliefs = Prior.beliefs_1,
    Final_beliefs = Final.beliefs_1,
    Credibility = Credibility_1,
    Confidence = Confidence_1,
    Bias = Sources.Variability_1,
    Error = Sources.Variability_4,
    Discretion = Sources.Variability_5
  ) %>% 
  select(ID, Condition, Attention_check, Prior_beliefs:Discretion)

df[,4:ncol(df)] <- sapply(df[,4:ncol(df)], as.numeric) #recode variables to numeric

# Function to show inline results
results <- function(Predictor, Multi_Condition = "Consistent") {
  linear_mod <- lm(
    pull(df, {{ Predictor }}) ~ Condition + Prior_beliefs, data = df)
  
  apa_results <- apa_print(linear_mod)
  
  ifelse(Multi_Condition == "Consistent",
         paste(apa_results$estimate$ConditionMulti_consistent,
               apa_results$statistic$ConditionMulti_consistent,
               sep = ", "),
         paste(apa_results$estimate$ConditionMulti_inconsistent,
               apa_results$statistic$ConditionMulti_inconsistent,
               sep = ", ")
         )
}

# Post-hoc t-test
df_multiconsistent <- df %>% 
  filter(Condition == "Multi-consistent")

t_test <- t.test(df_multiconsistent$Final_beliefs, df_multiconsistent$Prior_beliefs, paired = TRUE)
t_apa <- apa_print(t_test)
```

After two weeks of data collection, we recorded `r n_recorded` responses in Qualtrics. We excluded `r n_more_than_once` observations from participants who attempted to take the survey more than once, `r n_screened_out` participants who were screened out prior to starting the survey or did not consent, and `r n_failed_attention` participants who failed the attention check. This left us with a total sample of `r nrow(df)` participants (`r ns_bycondition[1]`, `r ns_bycondition[2]`, and `r ns_bycondition[3]` in the single-analyst, multi-consistent, and multi-inconsistent condition, respectively).     
Our main findings are displayed in Figure 1. Controlling for prior beliefs and comparing to the single-analyst condition, we found that (1) reported posterior beliefs were significantly lower in both the multi-consistent condition, `r results(Final_beliefs)`, and the multi-inconsistent condition, `r results(Final_beliefs, "Inconsistent")`; (2) ratings of credibility were significantly lower in the multi-inconsistent condition, `r results(Credibility, "Inconsistent")`, while they were not significantly different in the multi-consistent condition, `r results(Credibility)`; (3) confidence in the effect size estimate was significantly lower in the multi-inconsistent condition, `r results(Confidence, "Inconsistent")`, while it was not significantly different in the multi-consistent condition, `r results(Confidence)`; (4) ratings of bias were significantly greater in the multi-inconsistent condition, `r results(Bias, "Inconsistent")`, while they were not significantly different in the multi-consistent condition, `r results(Bias)`; and (5) ratings of error were significantly greater in both the multi-consistent condition, `r results(Error)`, and the multi-inconsistent condition, `r results(Error, "Inconsistent")`. For our exploratory measure of discretion, we found that ratings of experimenter degrees of freedom were significantly greater in both the multi-consistent condition, `r results(Discretion)`, and the multi-inconsistent condition, `r results(Discretion, "Inconsistent")`.   
**[Insert Figure 1 here]**  
In line with our hypotheses, lay consumers of multi-analyst studies with inconsistent results (compared to single-analyst studies) have lower posterior beliefs, find the results less credible, have less confidence in the average effect size estimate, and believe the results are more likely to stem from bias and error. Contrary to our hypotheses, we do not find that lay consumers of multi-analyst studies with consistent results (compared to single-analyst studies) have higher posterior beliefs, find the results more credible, have less confidence in the average effect size estimate, and believe the results are less likely to stem from bias and error: instead, they report significantly lower posterior beliefs and are more likely to believe the results stem from error (we did not find significant effects on ratings of credibility, confidence, or bias). Figure 2 further clarifies the sway of multi-analyst vs. single-analyst studies, by displaying the distribution of prior and posterior beliefs across the three conditions.  
**[Insert Figure 2 here]**  
It is worth noting on the basis of Figure 2 and a post-hoc, paired *t*-test that, while multi-analyst studies with consistent results perform worse or no better than single-analyst studies on all measures, there is a significant, positive effect of the findings on posterior beliefs *within* the multi-consistent condition: i.e., beliefs in the research hypothesis are greater after reading the multi-consistent study results, `r t_apa$full_result`. This finding clarifies that multi-analyst studies are not necessarily bad in absolute terms --- however, when comparing to conventional, single-analyst scientific research, crowdsourced data analysis does not seem to provide an improvement in the sway and credibility of scientific research to lay consumers.  

# Discussion

From the proliferation of big team science and large-scale replication initiatives to preregistration and registered reports, several scientific fields have undergone significant reform with the well-intended goal of improving the reliability of scientific research. The multi-analyst approach comes with many worthy uses, from demonstrating the arbitrariness and impact of individual analytic choices to acknowledging the inherent variability of results and averaging across idiosyncratic analytic choices to obtain more accurate parameter estimates. However, as with any real-world intervention, scientific reform can have unintended consequences. Here, we focus on the effects of crowdsourcing data analysis, and find that the multi-analyst approach may have an unintended consequence.  
While partly instituted with the goal of improving the credibility of scientific research (as shown in Table 1), lay consumers appear to resist the variability and lack of consensus that often comes with multi-analyst research. To our surprise, even when results generated by independent analysts are largely consistent, lay consumers are less likely to believe in the reported phenomenon and more likely to think that the findings stem from error and experimenter degrees of freedom. 

## Acknowledgements
We thank Nicole Clare Kolmstetter for valuable assistance in the data collection. This manuscript was created using `r cite_r("r-references.bib")`. 

## Disclosure statement
There are no relevant financial or non-financial competing interests to report.

## Data availability statement
The data that support the findings of this study are openly available on GitHub at https://github.com/shilaan/many-analysts and the OSF at https://osf.io/vedb4/.   

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage

# Tables

## Table 1
## *Proposed credibility- and confidence-related benefits of crowdsourced data analysis*

| Citation | Description of credibility- or confidence-related benefits | 
| ------------- | ------------- | 
| [@aczel2021, pp. 3-4] | "When the results of independent data analyses converge, more confidence in the conclusions is warranted. When the results diverge, confidence appropriately falters, and scientists can examine the reasons for these discrepancies."| 
|[@arbon2019] | "For the public to have faith in the conclusions of scientists it is important that the methods they employ are robust and transparent. This study will examine robustness by recruiting teams of independent data analysts and looking at how they answer a controversial research question using the same data, effectively ‘crowd-sourcing’ the data analysis." |
| [@auspurg2021, pp. 1, 10]  | "Several researchers analyze the same research question with the same data (...) Science is credible if different researchers come up with a similar answer"  | 
| [@breznau2021, p. 3] | "Organized scientific knowledge production (...) should generate inter-researcher reliability, offering consumers of scientific findings assurance that they are not arbitrary flukes but that other researchers would generate similar findings given the same data." | 
| [@breznau2021a, p. 311] | "crowdsourcing provides a new way to increase credibility for political and social research---in both sample populations and among the researchers themselves (...) It is hoped that these developments are tangible outcomes that increase public, private, and government views of social science." | 
|[@silberzahn2015, pp. 190-1]|"the results are more credible (...) greater certainty comes from having an independent check."|
|[@silberzahn2018, p. 352]|"Scientists can have comparatively more faith in a finding when there is less variability in analytic approaches taken to investigating the targeted phenomenon and in results obtained using different methods. (...) In such extreme cases of little to no convergence in results, the crowdsourcing process suggests that the scientific community should have no faith that the hypothesis is true"|
|[@uhlmann2019, p. 713]|"crowdsourced teams can conduct high-powered, precise studies and draw confident conclusions. (...) Crowdsourcing research is a part of a changing landscape of science that seeks to improve research reliability and advance the credibility of academic research"|

\newpage

## Table 2
*Predicted direction of effects compared to the single-analyst condition*

| Measure | Multi-consistent | Multi-inconsistent |
| ------------- |  ------------- | ------------- | 
| 1. Posterior beliefs | `r icon_style(icons::fontawesome("user-plus"), scale = 1.5, fill = "green")`  | `r icon_style(icons::fontawesome("user-minus"), scale = 1.5, fill = "red")` |
|2. Credibility | `r icon_style(icons::fontawesome("user-plus"), scale = 1.5, fill = "green")` | `r icon_style(icons::fontawesome("user-minus"), scale = 1.5, fill = "red")` |
| 3. Confidence  | `r icon_style(icons::fontawesome("user-minus"), scale = 1.5, fill = "red")`  | `r icon_style(icons::fontawesome("user-minus"), scale = 1.5, fill = "red")` | 
| 4. Bias | `r icon_style(icons::fontawesome("user-minus"), scale = 1.5, fill = "red")` | `r icon_style(icons::fontawesome("user-plus"), scale = 1.5, fill = "green")` |
| 5. Error | `r icon_style(icons::fontawesome("user-minus"), scale = 1.5, fill = "red")` | `r icon_style(icons::fontawesome("user-plus"), scale = 1.5, fill = "green")` |
| 6. Discretion | No prediction | No prediction |
*Note*. Table 2 indicates the predicted direction of the effect for all dependent variables, compared to the single-analyst condition and controlling for prior beliefs (a green plus stands for a positive prediction, and a red minus stands for a negative prediction). For example, we hypothesized that, compared to a single-analyst study and controlling for prior beliefs, ratings of credibility would be greater in the multi-consistent condition and lower in the multi-inconsistent condition.  

\newpage

# Figures

## Figure 1
*Estimates and 95% Confidence Intervals for all outcomes*
\noindent
```{r fig-1, echo=FALSE}
tidy_df <- function(Predictor) {
  linear_mod <- lm(
    pull(df, {{ Predictor }}) ~ Condition + Prior_beliefs, data = df)
  
  tidy_lm <- tidy(linear_mod, conf.int = TRUE) %>% 
    mutate(Measure = glue("{Predictor}")) %>% 
    filter(term != "Prior_beliefs" & term != "(Intercept)")
  tidy_lm
}

Predictors <- list("Final_beliefs", 
                  "Credibility", 
                  "Confidence", 
                  "Bias", 
                  "Error", 
                  "Discretion")

tidy_results <- Predictors %>% 
  map(~ tidy_df(.x)) %>% 
  bind_rows() %>% 
  mutate(term = gsub("Condition", "", term),
         Measure = factor(Measure,
                          levels = c("Final_beliefs",
                                     "Credibility",
                                     "Confidence",
                                     "Bias",
                                     "Error",
                                     "Discretion"))) %>% 
  mutate(Conclusion = case_when(
    conf.low < 0 & conf.high < 0 ~ "Negative",
    conf.low < 0 & conf.high > 0 ~ "Inconclusive",
    conf.low > 0 & conf.high > 0 ~ "Positive"
  ))

Measure.label = c("Posterior beliefs", Predictors %>% unlist() %>% tail(5))
names(Measure.label) <- Predictors %>% unlist()

p1 <- ggplot(mapping = aes(x = term, 
                           y = estimate,
                           color = Conclusion), 
             data = tidy_results) +
  geom_point(size = 2) +
  geom_errorbar(mapping = aes(ymin = conf.low, ymax = conf.high), 
                size = 0.8, width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray34", alpha = 0.7) +
  facet_wrap(~ Measure, 
             labeller = labeller(Measure = Measure.label)) + 
  theme_bw() + 
  theme(strip.background = element_rect(fill = "white"),
        legend.position = "",
        text = element_text(size = 16, 
                            family = "serif")) +
  labs(x = "Condition", y = "Estimate and 95% Confidence Interval") +
   scale_x_discrete(labels = c("Multi-\nconsistent", "Multi-\ninconsistent"))  +
  scale_color_discrete(type = c("darkgrey", "red", "green4"))

ggsave("Figure1.jpg", p1, 
       width = 7, height = 5, units = "in") #save to manuscript folder

ggsave(paste0(here(), "/talk/images/Figure1.jpg"), p1, 
       width = 7, height = 5, units = "in") #save to talk folder

ggsave(paste0(here(), "/poster/Figure1.jpg"), p1, 
       width = 7, height = 5, units = "in") #save to poster folder

knitr::include_graphics("Figure1.jpg", dpi = 1200)
```
*Note.* Figure 1 displays coefficient estimates (and 95% confidence intervals) of posterior beliefs, credibility, confidence, bias, error, and discretion in the two multi-analyst conditions, compared to the single-analyst condition (and controlling for prior beliefs). Green/red/gray error bars indicate positive/negative/insignificant findings, respectively.   

\newpage

## Figure 2

*Individual data points, quartiles, and distributions of prior and posterior beliefs in the single-analyst, multi-consistent, and multi-inconsistent conditions*  
\noindent
```{r raincloud, echo=FALSE}
set.seed(341990)

df <- df %>% 
  group_by(Condition) %>% 
  slice_sample(n = 499)

df_2x3 <- data_2x2(
  array_1 = df %>% filter(Condition == "Multi-inconsistent") %>% pull(Prior_beliefs),
  array_2 = df %>% filter(Condition == "Multi-inconsistent") %>% pull(Final_beliefs),
  array_3 = df %>% filter(Condition == "Multi-consistent") %>% pull(Prior_beliefs),
  array_4 = df %>% filter(Condition == "Multi-consistent") %>% pull(Final_beliefs),
  array_5 = df %>% filter(Condition == "Single") %>% pull(Prior_beliefs),
  array_6 = df %>% filter(Condition == "Single") %>% pull(Final_beliefs),
  labels = (c('Prior Beliefs','Final Beliefs')),
  jit_distance = .09,
  jit_seed = 321) 

colors <- rep(c("#035AA6", "darkorange"), 3) #choose colors 

p2 <- raincloud_2x3_repmes(
  data = df_2x3,
  colors = colors,
  fills = colors,
  size = 1,
  alpha = .7,
  ort = "h") + #set to v for vertical plot
  
  scale_x_continuous(
    breaks = c(1,2,3), 
    limits = c(0.8, 4.3), 
    labels = rep("", 3)) +
  ylab("Rated Beliefs") +
  
  annotate(geom = "text", 
           label = "Single-analyst", 
           x = 3.8, y = 13, size = 6,
           family = "serif") + 
  annotate(geom = "text", 
           label = "Multi-consistent", 
           x = 2.73, y = 14, size = 6,
           family = "serif") + 
  annotate(geom = "text", 
           label = "Multi-inconsistent",
           x = 1.78, y = 16, size = 6,
           family = "serif") + 
  annotate(geom = "text", 
           label = "Prior Beliefs", 
           fontface = "bold",
           x = 4.2, y = 26, size = 8, 
           color = "#035AA6",
           family = "serif") + 
  annotate(geom = "text", 
           label = "vs.", 
           x = 4.2, y = 45, size = 8,
           family = "serif") + 
  annotate(geom = "text", 
           label = "Posterior Beliefs", 
           fontface = "bold",
           x = 4.2, y = 68, size = 8, 
           color = "darkorange",
           family = "serif") + 
  
  theme_classic() +
  theme(axis.ticks.y = element_blank(),
        #axis.text = element_text(size = 9),
        axis.title.y = element_blank(),
        text = element_text(size = 16, family = "serif"))

ggsave("Figure2.jpg", p2, 
       width = 7, height = 5, units = "in") #save to manuscript folder

ggsave(paste0(here(), "/talk/images/Figure2.jpg"), p2, 
       width = 7, height = 5, units = "in") #save to talk folder

ggsave(paste0(here(), "/poster/Figure2.jpg"), p2, 
       width = 7, height = 5, units = "in") #save to poster folder

knitr::include_graphics("Figure2.jpg", dpi = 1200)
```
*Note. * Prior beliefs are displayed in blue; posterior beliefs are displayed in orange. The respective boxes display the lower quartiles, medians, and upper quartiles of prior and posterior beliefs by condition.

<!-- Given the existing huge inefficiencies, substantial improvements are almost certainly feasible...but neither presence nor absence of revolutionary intent should be taken as a reliable surrogate for actual impact…  -->

<!-- Interventions to change the current system should not be accepted without proper scrutiny, even when they are reasonable and well-intended.” -->

<!-- John Ioannidis -->
