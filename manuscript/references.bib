
@book{abelson1995,
  title = {Statistics {{As Principled Argument}}},
  author = {Abelson, Robert P},
  year = {1995},
  publisher = {{Taylor \& Francis}},
  address = {{New York}},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/8AL22UU2/Abelson - Statistics As Principled Argument.pdf}
}

@article{abelson1997,
  title = {On the {{Surprising Longevity}} of {{Flogged Horses}}: Why {{There Is}} a {{Case}} for the {{Significance Test}}},
  shorttitle = {On the {{Surprising Longevity}} of {{Flogged Horses}}},
  author = {Abelson, Robert P.},
  year = {1997},
  month = jan,
  journal = {Psychological Science},
  volume = {8},
  number = {1},
  pages = {12--15},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.1997.tb00536.x},
  abstract = {Criticisms of null-hypothesis significance tests (NHSTs) are reviewed Used as formal, two-valued decision procedures, they often generate misleading conclusions However, critics who argue that NHSTs are totally meaningless because the null hypothesis is virtually always false are overstating their case Critics also neglect the whole class of valuable significance tests that assess goodness of fit of models to data Even as applied to simple mean differences, NHSTs can be rhetorically useful in defending research against criticisms that random factors adequately explain the results, or that the direction of mean difference was not demonstrated convincingly Principled argument and counterargument produce the lore, or communal understanding, in a field, which in turn helps guide new research Alternative procedures\textendash confidence intervals, effect sizes, and meta-analysis\textendash are discussed Although these alternatives are not totally free from criticism either, they deserve more frequent use, without an unwise ban on NHSTs},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/TUZIUVQ2/Abelson - 1997 - On the Surprising Longevity of Flogged Horses Why.pdf}
}

@article{aczel2020,
  title = {Discussion Points for {{Bayesian}} Inference},
  author = {Aczel, Balazs and Hoekstra, Rink and Gelman, Andrew and Wagenmakers, Eric-Jan and Klugkist, Irene G. and Rouder, Jeffrey N. and Vandekerckhove, Joachim and Lee, Michael D. and Morey, Richard D. and Vanpaemel, Wolf and Dienes, Zoltan and {van Ravenzwaaij}, Don},
  year = {2020},
  month = jun,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {6},
  pages = {561--563},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0807-z},
  abstract = {Why is there no consensual way of conducting Bayesian analyses? We present a summary of agreements and disagreements of the authors on several discussion points regarding Bayesian inference. We also provide a thinking guideline to assist researchers in conducting Bayesian inference in the social and behavioural sciences.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Comments \& Opinion Subject\_term: Human behaviour;Science, technology and society Subject\_term\_id: human-behaviour;science-technology-and-society},
  file = {/Users/shilaan/Zotero/storage/WM8LJRU3/Aczel et al. - 2020 - Discussion points for Bayesian inference.pdf;/Users/shilaan/Zotero/storage/BY3QAIZR/s41562-019-0807-z.html}
}

@misc{aczel2021,
  title = {Guidance for Conducting and Reporting Multi-Analyst Studies},
  author = {Aczel, Balazs and Szaszi, Barnabas and Nilsonne, Gustav and van den Akker, Olmo and Albers, Casper and van Assen, Marcel A. L. M. and Bastiaansen, Jojanneke A. and Benjamin, Daniel Jacob and Boehm, Udo and {Botvinik-Nezer}, Rotem and Bringmann, Laura and Busch, Niko and Caruyer, Emmanuel and Cataldo, Andrea M. and Cowan, Nelson and Delios, Andrew and van Dongen, Noah and Donkin, Chris and van Doorn, Johnny and Almenberg, Anna Dreber and Dutilh, Gilles and Egan, Gary F. and Gernsbacher, Morton Ann and Hoekstra, Rink and Hoffmann, Sabine and Holzmeister, Felix and Johannesson, Magnus and Jonas, Kai and Kindel, Alexander and Kirchler, Michael and Kunkels, Yoram Kevin and StephenLindsay, D. and Mangin, Jan-Francois and Matzke, Dora and Munafo, Marcus and Newell, Ben and Nosek, Brian A. and Poldrack, Russell and van Ravenzwaaij, Don and Rieskamp, J{\"o}rg and Salganik, Matthew and Sarafoglou, Alexandra and Schonberg, Tom and Schweinsberg, Martin and Shanks, David and Silberzahn, Raphael and Simons, Daniel J. and Spellman, Bobbie and Starns, Jeffrey and {St-Jean}, Samuel and Uhlmann, Eric Luis and Wicherts, Jelte and Wagenmakers, Eric-Jan},
  year = {2021},
  month = apr,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/5ecnh},
  abstract = {We present consensus-based guidance for conducting and documenting multi-analyst studies. We discuss why broader adoption of the multi-analyst approach will strengthen the robustness of results and conclusions in empirical sciences.},
  keywords = {analytical variability,expert consensus,many analyst,Other Social and Behavioral Sciences,robustness,Social and Behavioral Sciences},
  file = {/Users/shilaan/Zotero/storage/XAPXY5R9/Aczel et al. - 2021 - Guidance for conducting and reporting multi-analys.pdf}
}

@book{apa2020,
  title = {Publication {{Manual}} of the {{American Psychological Association}}},
  author = {APA},
  year = {2019},
  edition = {Seventh edition},
  publisher = {{American Psychological Association}},
  address = {{Washington}},
  abstract = {"The Publication Manual of the American Psychological Association, Seventh Edition is the official source for APA Style. With millions of copies sold worldwide in multiple languages, it is the style manual of choice for writers, researchers, editors, students, and educators in the social and behavioral sciences, natural sciences, nursing, communications, education, business, engineering, and other fields. Known for its authoritative, easy-to-use reference and citation system, the Publication Manual also offers guidance on choosing the headings, tables, figures, language, and tone that will result in powerful, concise, and elegant scholarly communication. It guides users through the scholarly writing process-from the ethics of authorship to reporting research through publication. The seventh edition is an indispensable resource for students and professionals to achieve excellence in writing and make an impact with their work. The seventh edition has been thoroughly revised and updated to reflect best practices in scholarly writing and publishing. All formats are in full color, with a new tabbed spiral version Improved ease of navigation, with many additional numbered sections to help users quickly locate answers to their questions Resources for students on writing and formatting annotated bibliographies, response papers, and other paper types as well as guidelines on citing course materials Dedicated chapter for new users of APA Style covering paper elements and format, including sample papers for both professional authors and student writers New chapter on journal article reporting standards that includes updates to reporting standards for quantitative research and the first-ever qualitative and mixed methods reporting standards in APA Style New chapter on bias-free language guidelines for writing about people with respect and inclusivity in areas including age, disability, gender, participation in research, race and ethnicity, sexual orientation, socioeconomic status, and intersectionality More than 100 new reference examples covering periodicals, books, audiovisual media, social media, webpages and websites, and legal resources More than 40 new sample tables and figures Expanded guidance on ethical writing and publishing practices, including how to ensure the appropriate level of citation, avoid plagiarism and self-plagiarism, and navigate the publication process Guidelines that support accessibility for all users, including simplified reference, in-text citation, and heading formats as well as additional font options"--},
  isbn = {978-1-4338-3215-4 978-1-4338-3216-1 978-1-4338-3217-8},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/LWVWXMZ3/American Psychological Association (Washington, District of Columbia) - 2019 - Publication manual of the American psychological a.pdf}
}

@article{arbon2019,
  title = {{{MAPS}}: Mapping the {{Analytical Paths}} of a {{Crowdsourced Data Analysis}}},
  shorttitle = {{{MAPS}}},
  author = {Arbon, Robert and Drax, Katie and Thurlby, Natalie and Timpson, Nicholas John and Northstone, Kate and Brown, Kate Robson and Kwong, Alex and Munafo, Marcus},
  year = {2019},
  month = feb,
  publisher = {{OSF}},
  doi = {10.17605/OSF.IO/7VQ3B},
  abstract = {Reproducibility project into analysing and visualising analytic flexibility. A collaboration between the Jean Golding Institute (JGI), Avon Longitudinal Study for Parents and Children (ALSPAC) and UK Reproducibility Network (UKRN).},
  langid = {american},
  file = {/Users/shilaan/Zotero/storage/V9AA3XSR/7vq3b.html}
}

@article{ash1992,
  title = {Historicizing {{Mind Science}}: Discourse, {{Practice}}, {{Subjectivity}}},
  shorttitle = {Historicizing {{Mind Science}}},
  author = {Ash, Mitchell G.},
  year = {1992},
  journal = {Science in Context},
  volume = {5},
  number = {2},
  pages = {193--207},
  issn = {1474-0664, 0269-8897},
  doi = {10.1017/S0269889700001150},
  abstract = {It is no longer necessary to defend current historiography of psychology against the strictures aimed at its early text book incarnations in the 1960s and 1970s. At that time, Robert Young (1966) and others denigrated then standard textbook histories of psychology for their amateurism and their justifications propaganda for specific standpoints in current psychology, disguised as history. Since then, at least some textbooks writers and working historians of psychology have made such criticisms their own (Leahey 1986; Furumoto 1989). The demand for textbook histories continues nonetheless. Psychology, at least in the United States, remains the only discipline that makes historical representations of itself in the form of ``history and systems'' courses an official part of its pedagogical canon, required, interestingly enough, for the license in clinical practice (see Ash 1983).1 In the meantime, the professionalization of scholarship in history of psychology has proceeded apace. All of the trends visible in historical and social studies of other sciences, as well as in general cultural and intellectual history, are noe present in the historical study of psychology. Yet despite the visibility and social importance of psychology's various applications, and the prominence of certain schools of psychological thought such as behaviorism and psychoanalysis in contemporary cultural and political debate, the historiography of psychology has continued to hold a marginal position in history and social studies of science.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/BKG5A4HC/39454438E65E48D9EF9870F229FA488F.html}
}

@article{auspurg2021,
  title = {Has the {{Credibility}} of the {{Social Sciences Been Credibly Destroyed}}? Reanalyzing the ``{{Many Analysts}}, {{One Data Set}}'' {{Project}}},
  shorttitle = {Has the {{Credibility}} of the {{Social Sciences Been Credibly Destroyed}}?},
  author = {Auspurg, Katrin and Br{\"u}derl, Josef},
  year = {2021},
  month = jan,
  journal = {Socius},
  volume = {7},
  pages = {1--14},
  publisher = {{SAGE Publications}},
  issn = {2378-0231},
  doi = {10.1177/23780231211024421},
  abstract = {In 2018, Silberzahn, Uhlmann, Nosek, and colleagues published an article in which 29 teams analyzed the same research question with the same data: Are soccer referees more likely to give red cards to players with dark skin tone than light skin tone? The results obtained by the teams differed extensively. Many concluded from this widely noted exercise that the social sciences are not rigorous enough to provide definitive answers. In this article, we investigate why results diverged so much. We argue that the main reason was an unclear research question: Teams differed in their interpretation of the research question and therefore used diverse research designs and model specifications. We show by reanalyzing the data that with a clear research question, a precise definition of the parameter of interest, and theory-guided causal reasoning, results vary only within a narrow range. The broad conclusion of our reanalysis is that social science research needs to be more precise in its ?estimands? to become credible.},
  file = {/Users/shilaan/Zotero/storage/3DXVYB98/Auspurg and Br√ºderl - 2021 - Has the Credibility of the Social Sciences Been Cr.pdf}
}

@book{bakan1967,
  title = {On Method: Toward a Reconstruction of Psychological Investigation.},
  shorttitle = {On Method},
  author = {Bakan, David},
  year = {1967},
  publisher = {{Jossey-Bass}},
  address = {{San Francisco}},
  isbn = {978-0-87589-008-1},
  langid = {english},
  annotation = {OCLC: 190982}
}

@article{bakker2012,
  title = {The {{Rules}} of the {{Game Called Psychological Science}}},
  author = {Bakker, Marjan and {van Dijk}, Annette and Wicherts, Jelte M.},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {543--554},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691612459060},
  abstract = {If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96\% of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p {$<$} .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.},
  file = {/Users/shilaan/Zotero/storage/LHTJ6EX5/Bakker et al. - 2012 - The Rules of the Game Called Psychological Science.pdf}
}

@techreport{beffarabret2018,
  type = {Preprint},
  title = {A Fully Automated, Transparent, Reproducible, and Blind Protocol for Sequential Analyses},
  author = {Beffara Bret, Brice and Beffara Bret, Am{\'e}lie and Nalborczyk, Ladislas},
  year = {2018},
  month = feb,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/v7xpg},
  abstract = {Despite many cultural, methodological and technical improvements, one of the major obstacle to results reproducibility remains the pervasive low statistical power. In response to this problem, a lot of attention has recently been drawn to sequential analyses. This type of procedure has been shown to be more efficient (to require less observations and therefore less resources) than classical fixed-N procedures. However, these procedures are submitted to both intrapersonal and interpersonal biases during data collection and data analysis. In this tutorial, we explain how automation can be used to prevent these biases. We show how to synchronise open and free experiment software programs with the Open Science Framework and how to automate sequential data analyses in R. This tutorial is intended to researchers with beginner experience with R but no previous experience with sequential analyses is required.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/3BJNPBI3/Beffara Bret et al. - 2018 - A fully automated, transparent, reproducible, and .pdf}
}

@article{bem2011,
  title = {Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl J.},
  year = {2011},
  journal = {Journal of Personality and Social Psychology},
  volume = {100},
  number = {3},
  pages = {407--425},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1315(Electronic),0022-3514(Print)},
  doi = {10.1037/a0021524},
  abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by ``time-reversing'' well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Causality,Cognition,Cognitions,Emotional States,Extrasensory Perception,Parapsychology,Precognition},
  file = {/Users/shilaan/Zotero/storage/G2WGGKTT/2011-01894-001.html}
}

@article{biesanz2017,
  title = {Expected Statistical Power: Justifying and Evaluating Statistical Power},
  author = {Biesanz, Jeremy},
  year = {2017},
  journal = {Society for the Improvement of Psychological Science Meeting},
  publisher = {{OSF}},
  doi = {None},
  abstract = {Workshop materials, slides, examples, and R code      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/CCA2T624/4wvbe.html}
}

@article{blume2002,
  title = {Likelihood Methods for Measuring Statistical Evidence},
  author = {Blume, Jeffrey D.},
  year = {2002},
  month = sep,
  journal = {Statistics in Medicine},
  volume = {21},
  number = {17},
  pages = {2563--2599},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.1216},
  abstract = {Focused on interpreting data as statistical evidence, the evidential paradigm uses likelihood ratios to measure the strength of statistical evidence. Under this paradigm, re-examination of accumulating evidence is encouraged because (i) the likelihood ratio, unlike a p-value, is una ected by the number of examinations and (ii) the probability of observing strong misleading evidence is naturally low, even for study designs that re-examine the data with each new observation. Further, the controllable probabilities of observing misleading and weak evidence provide assurance that the study design is reliable without a ecting the strength of statistical evidence in the data. This paper illustrates the ideas and methods associated with using likelihood ratios to measure statistical evidence. It contains a comprehensive introduction to the evidential paradigm, including an overview of how to quantify the probability of observing misleading evidence for various study designs. The University Group Diabetes Program (UGDP), a classic and still controversial multi-centred clinical trial, is used as an illustrative example. Some of the original UGDP results, and subsequent re-analyses, are presented for comparison purposes. Copyright ? 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/S38HJBZG/Blume - 2002 - Likelihood methods for measuring statistical evide.pdf}
}

@misc{breznau2021,
  title = {Observing {{Many Researchers Using}} the {{Same Data}} and {{Hypothesis Reveals}} a {{Hidden Universe}} of {{Uncertainty}}},
  author = {Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Adem, Muna and Adriaans, Jule and {Alvarez-Benjumea}, Amalia and Andersen, Henrik Kenneth and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin and Castillo, Juan Carlos and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Am{\'e}lie and Gr{\"o}mping, Max and Gro{\ss}, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and H{\"o}vermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ignacz, Zsofia and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Ko{\l}czy{\'n}ska, Marta and Kuk, John Seungmin and Kuni{\ss}en, Katharina and Sinatra, Dafina Kurti and Greinert, Alexander and Lersch, Philipp M. and L{\"o}bel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar Jose and McManus, Patricia and Wagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan J. B. and Moya, Crist{\'o}bal and Neunhoeffer, Marcel and N{\"u}st, Daniel and Nyg{\aa}rd, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Katja M. and Schmidt, Regine and {Schmidt-Catran}, Alexander and Schmiedeberg, Claudia and Schneider, J{\"u}rgen and Schoonvelde, Martijn and {Schulte-Cloos}, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, J{\"u}rgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian B. and Vagni, Giacomo and Assche, Jasper Van and van der Linden, Meta and van der Noll, Jolanda and Hootegem, Arno Van and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and {\.Z}{\'o}{\l}tak, Tomasz and Nguyen, Hung H. V.},
  year = {2021},
  month = mar,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/cd5j9},
  abstract = {How does noise generated by researcher decisions undermine the credibility of science? We test this by observing all decisions made among 73 research teams as they independently conduct studies on the same hypothesis with identical starting data. We find excessive variation of outcomes. When combined, the 107 observed research decisions taken across teams explained at most 2.6\% of the total variance in effect sizes and 10\% of the deviance in subjective conclusions. Expertise, prior beliefs and attitudes of the researchers explain even less. Each model deployed to test the hypothesis was unique, which highlights a vast universe of research design variability that is normally hidden from view and suggests humility when presenting and interpreting scientific findings.},
  keywords = {Analytical Flexibility,Crowdsourced Replication Initiative,Crowdsourcing,Economics,Garden of Forking Paths,Immigration,Many Analysts,Meta-Science,Noise,Other Social and Behavioral Sciences,Political Science,Psychology,Researcher Degrees of Freedom,Researcher Variability,Social and Behavioral Sciences,Social Policy,Sociology},
  file = {/Users/shilaan/Zotero/storage/H3JLBJF6/Breznau et al. - 2021 - Observing Many Researchers Using the Same Data and.pdf}
}

@article{breznau2021a,
  title = {I {{Saw You}} in the {{Crowd}}: Credibility, {{Reproducibility}}, and {{Meta}}-{{Utility}}},
  shorttitle = {I {{Saw You}} in the {{Crowd}}},
  author = {Breznau, Nate},
  year = {2021},
  month = apr,
  journal = {PS: Political Science \& Politics},
  volume = {54},
  number = {2},
  pages = {309--313},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096520000980},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1049096520000980/resource/name/firstPage-S1049096520000980a.jpg},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/5NUAHU29/Breznau - 2021 - I Saw You in the Crowd Credibility, Reproducibili.pdf;/Users/shilaan/Zotero/storage/EV7EVN4Q/61AE9514D02A06DC111FB3E0E591F491.html}
}

@article{button2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/PPE8VAE3/Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@article{cohen1962,
  title = {The Statistical Power of Abnormal-Social Psychological Research: A Review},
  shorttitle = {The Statistical Power of Abnormal-Social Psychological Research},
  author = {Cohen, Jacob},
  year = {1962},
  journal = {The Journal of Abnormal and Social Psychology},
  volume = {65},
  number = {3},
  pages = {145--153},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {0096-851X(Print)},
  doi = {10.1037/h0045186},
  file = {/Users/shilaan/Zotero/storage/KCJS98U3/1964-09448-001.html}
}

@article{cohen1994,
  title = {The Earth Is Round (p\hspace{0.6em}{$<$}\hspace{0.6em}.05)},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H{$_0$} is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H{$_0$} one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing},
  file = {/Users/shilaan/Zotero/storage/9NG782LN/Cohen - 1994 - The earth is round (p  .05).pdf;/Users/shilaan/Zotero/storage/AFMRDMUJ/1995-12080-001.html}
}

@article{coon1993,
  title = {Standardizing the {{Subject}}: Experimental {{Psychologists}}, {{Introspection}}, and the {{Quest}} for a {{Technoscientific Ideal}}},
  shorttitle = {Standardizing the {{Subject}}},
  author = {Coon, Deborah J.},
  year = {1993},
  journal = {Technology and Culture},
  volume = {34},
  number = {4},
  pages = {757--783},
  issn = {0040-165X},
  doi = {10.2307/3106414}
}

@article{danziger1985,
  title = {The {{Methodological Imperative}} in {{Psychology}}},
  author = {Danziger, Kurt},
  year = {1985},
  month = mar,
  journal = {Philosophy of the Social Sciences},
  volume = {15},
  number = {1},
  pages = {1--13},
  publisher = {{SAGE Publications Inc}},
  issn = {0048-3931},
  doi = {10.1177/004839318501500101},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/ZZP8GUUJ/Danziger - 1985 - The Methodological Imperative in Psychology.pdf}
}

@book{danziger1990,
  title = {Constructing the Subject:  Historical Origins of Psychological Research},
  shorttitle = {Constructing the Subject},
  author = {Danziger, Kurt},
  year = {1990},
  series = {Constructing the Subject:  Historical Origins of Psychological Research},
  pages = {ix, 254},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, US}},
  doi = {10.1017/CBO9780511524059},
  abstract = {"Constructing the Subject" traces the history of psychological research methodology from the nineteenth century to the emergence of currently favored styles of research in the second quarter of the twentieth century. Kurt Danziger considers methodology to be a kind of social practice rather than simply a matter of technique. Therefore his historical analysis is primarily concerned with such topics as the development of the social structure of the research relationship between experimenters and their subjects, as well as the role of the methodology in the relationship of investigators to each other in a wider social context. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-521-36358-7 978-0-521-46785-8},
  keywords = {Experimentation,History of Psychology,Methodology,Psychosocial Factors},
  file = {/Users/shilaan/Zotero/storage/BWER3N3V/1994-97559-000.html}
}

@article{dienes2011,
  title = {Bayesian {{Versus Orthodox Statistics}}: Which {{Side Are You On}}?},
  shorttitle = {Bayesian {{Versus Orthodox Statistics}}},
  author = {Dienes, Zoltan},
  year = {2011},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {6},
  number = {3},
  pages = {274--290},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691611406920},
  abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing\textemdash two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/3A4FHA5X/Dienes - 2011 - Bayesian Versus Orthodox Statistics Which Side Ar.pdf}
}

@article{dienes2016,
  title = {How {{Bayes}} Factors Change Scientific Practice},
  author = {Dienes, Zoltan},
  year = {2016},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  volume = {72},
  pages = {78--89},
  issn = {00222496},
  doi = {10.1016/j.jmp.2015.10.003},
  abstract = {Bayes factors provide a symmetrical measure of evidence for one model versus another (e.g. H1 versus H0) in order to relate theory to data. These properties help solve some (but not all) of the problems underlying the credibility crisis in psychology. The symmetry of the measure of evidence means that there can be evidence for H0 just as much as for H1; or the Bayes factor may indicate insufficient evidence either way. P-values cannot make this three-way distinction. Thus, Bayes factors indicate when the data count against a theory (and when they count for nothing); and thus they indicate when replications actually support H0 or H1 (in ways that power cannot). There is every reason to publish evidence supporting the null as going against it, because the evidence can be measured to be just as strong either way (thus the published record can be more balanced). Bayes factors can be B-hacked but they mitigate the problem because a) they allow evidence in either direction so people will be less tempted to hack in just one direction; b) as a measure of evidence they are insensitive to the stopping rule; c) families of tests cannot be arbitrarily defined; and d) falsely implying a contrast is planned rather than post hoc becomes irrelevant (though the value of pre-registration is not mitigated).},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/L563QCHY/Dienes - 2016 - How Bayes factors change scientific practice.pdf}
}

@article{dodge1929,
  title = {A {{Method}} of {{Sampling Inspection}}},
  author = {Dodge, H. F. and Romig, H. G.},
  year = {1929},
  month = oct,
  journal = {Bell System Technical Journal},
  volume = {8},
  number = {4},
  pages = {613--631},
  issn = {00058580},
  doi = {10.1002/j.1538-7305.1929.tb01240.x},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/VHXRWEPD/Dodge and Romig - 1929 - A Method of Sampling Inspection.pdf}
}

@article{earp2015,
  title = {Replication, Falsification, and the Crisis of Confidence in Social Psychology},
  author = {Earp, Brian D. and Trafimow, David},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {{Frontiers}},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00621},
  abstract = {The (latest) ``crisis in confidence'' in social psychology has generated much heated discussion about the importance of replication, including how such replication should be carried out as well as interpreted by scholars in the field. What does it mean if a replication attempt ``fails''\textemdash does it mean that the original results, or the theory that predicted them, have been falsified? And how should ``failed'' replications affect our belief in the validity of the original research? In this paper, we consider the ``replication'' debate from a historical and philosophical perspective, and provide a conceptual analysis of both replication and falsification as they pertain to this important discussion. Along the way, we introduce a Bayesian framework for assessing ``failed'' replications in terms of how they should affect our confidence in purported findings.},
  langid = {english},
  keywords = {crisis of replicability,Falsification,Philosophy of science,Psychology,Replication,Social},
  file = {/Users/shilaan/Zotero/storage/JK7R9UH8/Earp and Trafimow - 2015 - Replication, falsification, and the crisis of conf.pdf}
}

@article{fan2004,
  title = {Conditional {{Bias}} of {{Point Estimates Following}} a {{Group Sequential Test}}},
  author = {Fan, Xiaoyin and DeMets, David L. and Lan, K. K. Gordon},
  year = {2004},
  month = dec,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {14},
  number = {2},
  pages = {505--530},
  issn = {1054-3406, 1520-5711},
  doi = {10.1081/BIP-120037195},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/UY8PJ5CJ/Fan et al. - 2004 - Conditional Bias of Point Estimates Following a Gr.pdf}
}

@article{fisher1922,
  title = {On the Mathematical Foundations of Theoretical Statistics},
  author = {Fisher, R. A.},
  year = {1922},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {222},
  number = {594-604},
  pages = {309--368},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.1922.0009},
  abstract = {Several reasons have contributed to the prolonged neglect into which the study of statistics, in its theoretical aspects, has fallen. In spite of the immense amount of fruitful labour which has been expended in its practical applications, the basic principles of this organ of science are still in a state of obscurity, and it cannot be denied that, during the recent rapid development of practical methods, fundamental problems have been ignored and fundamental paradoxes left unresolved. This anomalous state of statistical science is strikingly exemplified by a recent paper entitled "The Fundamental Problem of Practical Statistics," in which one of the most eminent of modern statisticians presents what purports to be a general proof of BAYES' postulate, a proof which, in the opinion of a second statistician of equal eminence, "seems to rest upon a very peculiar -- not to say hardly supposable -- relation."},
  file = {/Users/shilaan/Zotero/storage/XPUK5MSN/Fisher and Russell - 1922 - On the mathematical foundations of theoretical sta.pdf;/Users/shilaan/Zotero/storage/5VPZTTRH/rsta.1922.html}
}

@book{fisher1925,
  title = {Statistical Methods for Research Workers, 11th Ed. Rev},
  author = {Fisher, R.A.},
  year = {1925},
  series = {Statistical Methods for Research Workers, 11th Ed. Rev},
  publisher = {{Edinburgh}},
  address = {{Oliver and Boyd}},
  abstract = {Contains revisions of probability formulas and treatment of correlations.  Harvard Book List (edited) 1955 \#94 (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/shilaan/Zotero/storage/GIZ6ZH8W/1925-15003-000.html}
}

@book{fisher1935,
  title = {The Design of Experiments},
  author = {Fisher, R. A.},
  year = {1935},
  series = {The Design of Experiments},
  pages = {xi, 251},
  publisher = {{Oliver \& Boyd}},
  address = {{Oxford, England}},
  abstract = {Different types of experimentation are considered with reference to their logical structure, to show that valid conclusions may be drawn from them without using the disputed theory of inductive inferences, i.e., of arguing from observation to explanatory theory. This is possible if a null hypothesis is explicitly formulated when the experiment is designed; this hypothesis can never be proved, but may be disproved with whatever probability one will accept as demonstrating a positive result. Chapters II, III, and IV illustrate simple applications of the principles involved in sensitiveness, significance, tests of wider hypotheses, validity, and estimation and elimination of error. More elaborate structures are treated in later chapters. Chapter titles are: (V) the Latin square; (VI) factorial design in experimentation; (VII) confounding; (VIII) special cases of partial confounding; (IX) increase of precision by concomitant measurements: statistical control; (X) generalization of null hypotheses: fiducial probability; (XI) measurement of amount of information in general. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/shilaan/Zotero/storage/4F6EVRKZ/1939-04964-000.html}
}

@book{fisher1956,
  title = {Statistical Methods and Scientific Inference},
  author = {Fisher, Ronald A.},
  year = {1956},
  series = {Statistical Methods and Scientific Inference},
  pages = {viii, 175},
  publisher = {{Hafner Publishing Co.}},
  address = {{Oxford, England}},
  abstract = {An explicit statement of the logical nature of statistical reasoning that has been implicitly required in the development and use of statistical techniques in the making of uncertain inferences and in the design of experiments. Included is a consideration of the concept of mathematical probability; a comparison of fiducial and confidence intervals; a comparison of the logic of tests of significance with the acceptance decision approach; and a discussion of the principles of prediction and estimation. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{fisher1999,
  title = {Carvedilol and the {{Food}} and {{Drug Administration}} Approval Process: An Introduction},
  shorttitle = {Carvedilol and the {{Food}} and {{Drug Administration}} Approval Process},
  author = {Fisher, L. D. and Moy{\'e}, L. A.},
  year = {1999},
  month = feb,
  journal = {Controlled Clinical Trials},
  volume = {20},
  number = {1},
  pages = {1--15},
  issn = {0197-2456},
  doi = {10.1016/s0197-2456(98)00052-x},
  abstract = {We discuss briefly the new drug carvedilol (Coreg), a beta-blocker, alpha-blocker, and antioxidant. This drug was developed for congestive heart failure in a series of trials, four in the United States and one in Australia and New Zealand, briefly summarized in this document. We also summarize the classical paradigm of the U.S. Food and Drug Administration (FDA) for drug approval and the FDA's use of advisory committees. This document serves as background to the discussion of carvedilol's approval.},
  langid = {english},
  pmid = {10027497},
  keywords = {Adrenergic alpha-Antagonists,Adrenergic beta-Antagonists,Carbazoles,Carvedilol,Drug Approval,Heart Failure,Humans,Propanolamines,Public Policy,Randomized Controlled Trials as Topic,United States,United States Food and Drug Administration}
}

@article{fisher1999a,
  title = {Carvedilol and the {{Food}} and {{Drug Administration}} ({{FDA}}) Approval Process: The {{FDA}} Paradigm and Reflections on Hypothesis Testing},
  shorttitle = {Carvedilol and the {{Food}} and {{Drug Administration}} ({{FDA}}) Approval Process},
  author = {Fisher, L. D.},
  year = {1999},
  month = feb,
  journal = {Controlled Clinical Trials},
  volume = {20},
  number = {1},
  pages = {16--39},
  issn = {0197-2456},
  doi = {10.1016/s0197-2456(98)00054-3},
  abstract = {Carvedilol (Coreg), a beta- and alpha-blocker and an antioxidant drug, was evaluated for moderate to severe heart failure patients in a program containing four United States and one Australia/New Zealand study. The data were evaluated twice by the Cardiovascular and Renal Drugs Advisory Committee of the U.S. Food and Drug Administration (FDA). These meetings resulted in opposite decisions by the advisory committee. The crux of the argumentation was the two-positive-trial FDA paradigm. Carvedilol did not meet the usual paradigm because an exercise end point was not statistically different from placebo in three U.S. trials. Most other end points were highly significant, and death, which was monitored across the U.S. program, was different with p {$<$} 0.0001. Here we argue that the usual paradigm is very useful but not an absolute principle, that the usual paradigm can sometimes miss the strength of evidence even in the primary end points, and that rational decision making requires on occasion that other evidence must lead to approval. Control of the type I error rate should be taken very seriously, should rarely be violated, and serves the biomedical community well. It is not an absolute principle, however, but rather must be considered in context.},
  langid = {english},
  pmid = {10027498},
  keywords = {Adrenergic alpha-Antagonists,Adrenergic beta-Antagonists,Carbazoles,Carvedilol,Drug Approval,Heart Failure,Humans,Propanolamines,Public Policy,Randomized Controlled Trials as Topic,United States,United States Food and Drug Administration}
}

@article{fleming1984,
  title = {Designs for Group Sequential Tests},
  author = {Fleming, Thomas R. and Harrington, David P. and O'Brien, Peter C.},
  year = {1984},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {5},
  number = {4},
  pages = {348--361},
  issn = {01972456},
  doi = {10.1016/S0197-2456(84)80014-8},
  abstract = {Several authors have proposed group sequential procedures to satisfy the ethical need in clinical trials for interim analyses. We propose here an alternative procedure that offers a good opportunity for early termination when initial results are extreme, while essentially maintaining the power provided by the procedure that applies when the corresponding test statistic is computed only at the predetermined time of final analysis. The new design is also sufficiently flexible to readily allow one to increase the maximum number of analyses for certain reasons such as unexpectedly slow accrual into the study, although not for reasons arising from consideration of outcomes observed at interim analysis.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/8R2TFDBF/Fleming et al. - 1984 - Designs for group sequential tests.pdf}
}

@article{franzoni2014,
  title = {Crowd Science: The Organization of Scientific Research in Open Collaborative Projects},
  shorttitle = {Crowd Science},
  author = {Franzoni, Chiara and Sauermann, Henry},
  year = {2014},
  month = feb,
  journal = {Research Policy},
  volume = {43},
  number = {1},
  pages = {1--20},
  issn = {0048-7333},
  doi = {10.1016/j.respol.2013.07.005},
  abstract = {A growing amount of scientific research is done in an open collaborative fashion, in projects sometimes referred to as ``crowd science'', ``citizen science'', or ``networked science''. This paper seeks to gain a more systematic understanding of crowd science and to provide scholars with a conceptual framework and an agenda for future research. First, we briefly present three case examples that span different fields of science and illustrate the heterogeneity concerning what crowd science projects do and how they are organized. Second, we identify two fundamental elements that characterize crowd science projects \textendash{} open participation and open sharing of intermediate inputs \textendash{} and distinguish crowd science from other knowledge production regimes such as innovation contests or traditional ``Mertonian'' science. Third, we explore potential knowledge-related and motivational benefits that crowd science offers over alternative organizational modes, and potential challenges it is likely to face. Drawing on prior research on the organization of problem solving, we also consider for what kinds of tasks particular benefits or challenges are likely to be most pronounced. We conclude by outlining an agenda for future research and by discussing implications for funding agencies and policy makers.},
  langid = {english},
  keywords = {Citizen science,Community-based production,Crowd science,Crowdsourcing,Funding,Open innovation,Problem solving},
  file = {/Users/shilaan/Zotero/storage/I45HINJ2/Franzoni and Sauermann - 2014 - Crowd science The organization of scientific rese.pdf}
}

@incollection{garfield2004,
  title = {Research on {{Statistical Literacy}}, {{Reasoning}}, and {{Thinking}}: Issues, {{Challenges}}, and {{Implications}}},
  shorttitle = {Research on {{Statistical Literacy}}, {{Reasoning}}, and {{Thinking}}},
  booktitle = {The {{Challenge}} of {{Developing Statistical Literacy}}, {{Reasoning}} and {{Thinking}}},
  author = {Garfield, Joan and {Ben-Zvi}, Dani},
  editor = {{Ben-Zvi}, Dani and Garfield, Joan},
  year = {2004},
  pages = {397--409},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/1-4020-2278-6_17},
  abstract = {SummaryThis book focuses on one aspect of the ``infancy'' of the field of statistics education research, by attempting to grapple with the definitions, distinctions, and development of statistical literacy, reasoning, and thinking. As this field grows, the research studies in this volume should help provide a strong foundation as well as a common research literature. This is an exciting time, given the newness of the research area and the energy and enthusiasm of the contributing researchers and educators who are helping to shape the discipline as well as the future teaching and learning of statistics. We point out that there is room for more participants to help define and construct the research agenda and contribute to results. We hope to see many new faces at future gatherings of the international research community, whether at SRTL-4, or 5, or other venues such as the International Conference on Teaching Statistics (ICOTS), International Congresson Mathematical Education (ICME), and the International Group for the Psychology of Mathematics Education (PME).},
  isbn = {978-1-4020-2278-4},
  langid = {english},
  keywords = {International Statistical Institute,Statistical Literacy,Statistical Reasoning,Statistical Thinking,Technological Tool}
}

@article{gelman2016,
  title = {The {{Problems With P}}-{{Values}} Are Not {{Just With P}}-{{Values}}},
  author = {Gelman, Andrew},
  year = {2016},
  journal = {The American Statistician},
  number = {Online Discussion},
  pages = {1--2},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/2SYFPRTI/Gelman - The Problems With P-Values are not Just With P-Val.pdf}
}

@book{gigerenzer1987,
  title = {Cognition as Intuitive Statistics},
  author = {Gigerenzer, Gerd and Murray, David J.},
  year = {1987},
  series = {Cognition as Intuitive Statistics},
  pages = {xiii, 214},
  publisher = {{Lawrence Erlbaum Associates, Inc}},
  address = {{Hillsdale, NJ, US}},
  abstract = {Cognition as statistical inference\textemdash this is the topic of the present book.  In the first chapter, we discuss the rise of the inference revolution, which institutionalized those statistical tools that later became theories of cognitive processes. In each of the following four chapters we treat one major topic of cognitive psychology and show to what degree statistical concepts transformed our understanding of those topics. The topics are (a) detection and discrimination, the classical psychophysical problems; (b) perception, in particular the problem of how properties of objects are judged and classified; (c) memory, the problems of recognition and recall; and (d) thinking, in particular the problems of inductive reasoning and rationality. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-89859-570-3},
  keywords = {Cognitive Processes,Inductive Deductive Reasoning,Inference,Memory,Perception,Psychophysiology,Statistics},
  file = {/Users/shilaan/Zotero/storage/7CIH2M2E/1987-97295-000.html}
}

@book{gigerenzer1989,
  title = {The {{Empire}} of {{Chance}}: How {{Probability Changed Science}} and {{Everyday Life}}},
  shorttitle = {The {{Empire}} of {{Chance}}},
  author = {Gigerenzer, Gerd and Swijtink, Zeno and Porter, Theodore and Daston, Lorraine and Beatty, John and Kruger, Lorenz},
  year = {1989},
  series = {Ideas in {{Context}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511720482},
  abstract = {The Empire of Chance tells how quantitative ideas of chance transformed the natural and social sciences, as well as daily life over the last three centuries. A continuous narrative connects the earliest application of probability and statistics in gambling and insurance to the most recent forays into law, medicine, polling and baseball. Separate chapters explore the theoretical and methodological impact in biology, physics and psychology. Themes recur - determinism, inference, causality, free will, evidence, the shifting meaning of probability - but in dramatically different disciplinary and historical contexts. In contrast to the literature on the mathematical development of probability and statistics, this book centres on how these technical innovations remade our conceptions of nature, mind and society. Written by an interdisciplinary team of historians and philosophers, this readable, lucid account keeps technical material to an absolute minimum. It is aimed not only at specialists in the history and philosophy of science, but also at the general reader and scholars in other disciplines.},
  isbn = {978-0-521-39838-1},
  file = {/Users/shilaan/Zotero/storage/4JYNV9F4/9DAF0E94CEB7D88FB8E2BD52460AC70F.html}
}

@article{gigerenzer2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  year = {2004},
  month = nov,
  journal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the ``null ritual'' consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {Collective illusions,Editors,Rituals,Statistical significance,Textbooks},
  file = {/Users/shilaan/Zotero/storage/YG7HFKUR/Gigerenzer - 2004 - Mindless statistics.pdf;/Users/shilaan/Zotero/storage/YIZMZ3Y4/S1053535704000927.html}
}

@article{gigerenzer2015,
  title = {Surrogate {{Science}}: The {{Idol}} of a {{Universal Method}} for {{Scientific Inference}}},
  shorttitle = {Surrogate {{Science}}},
  author = {Gigerenzer, Gerd and Marewski, Julian N.},
  year = {2015},
  month = feb,
  journal = {Journal of Management},
  volume = {41},
  number = {2},
  pages = {421--440},
  publisher = {{SAGE Publications Inc}},
  issn = {0149-2063},
  doi = {10.1177/0149206314547522},
  abstract = {The application of statistics to science is not a neutral act. Statistical tools have shaped and were also shaped by its objects. In the social sciences, statistical methods fundamentally changed research practice, making statistical inference its centerpiece. At the same time, textbook writers in the social sciences have transformed rivaling statistical systems into an apparently monolithic method that could be used mechanically. The idol of a universal method for scientific inference has been worshipped since the ``inference revolution'' of the 1950s. Because no such method has ever been found, surrogates have been created, most notably the quest for significant p values. This form of surrogate science fosters delusions and borderline cheating and has done much harm, creating, for one, a flood of irreproducible results. Proponents of the ``Bayesian revolution'' should be wary of chasing yet another chimera: an apparently universal inference procedure. A better path would be to promote both an understanding of the various devices in the ``statistical toolbox'' and informed judgment to select among these.},
  langid = {english},
  keywords = {Bayesian methods,psychometrics,regression analysis,research methods},
  file = {/Users/shilaan/Zotero/storage/UJJFHAPX/Gigerenzer and Marewski - 2015 - Surrogate Science The Idol of a Universal Method .pdf}
}

@book{gigerenzer2015a,
  title = {Cognition as {{Intuitive Statistics}}},
  author = {Gigerenzer, Gerd and Murray, David J.},
  year = {2015},
  month = aug,
  publisher = {{Psychology Press}},
  abstract = {Originally published in 1987, this title is about theory construction in psychology. Where theories come from, as opposed to how they become established, was almost a no-man's land in the history and philosophy of science at the time. The authors argue that in the science of mind, theories are particularly likely to come from tools, and they are especially concerned with the emergence of the metaphor of the mind as an intuitive statistician. In the first chapter, the authors discuss the rise of the inference revolution, which institutionalized those statistical tools that later became theories of cognitive processes. In each of the four following chapters they treat one major topic of cognitive psychology and show to what degree statistical concepts transformed their understanding of those topics.},
  googlebooks = {NLJgCgAAQBAJ},
  isbn = {978-1-317-36218-0},
  langid = {english},
  keywords = {Psychology / Cognitive Psychology \& Cognition,Psychology / History,Psychology / Statistics}
}

@article{goodman1988,
  title = {Evidence and Scientific Research.},
  author = {Goodman, S N and Royall, R},
  year = {1988},
  month = dec,
  journal = {American Journal of Public Health},
  volume = {78},
  number = {12},
  pages = {1568--1574},
  issn = {0090-0036, 1541-0048},
  doi = {10.2105/AJPH.78.12.1568},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/VVNZAZCR/Goodman and Royall - 1988 - Evidence and scientific research..pdf}
}

@article{goodman1999,
  title = {Toward {{Evidence}}-{{Based Medical Statistics}}. 1: The {{P Value Fallacy}}},
  shorttitle = {Toward {{Evidence}}-{{Based Medical Statistics}}. 1},
  author = {Goodman, Steven N.},
  year = {1999},
  month = jun,
  journal = {Annals of Internal Medicine},
  volume = {130},
  number = {12},
  pages = {995},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-130-12-199906150-00008},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/T37IXRVZ/Goodman - 1999 - Toward Evidence-Based Medical Statistics. 1 The P.pdf}
}

@article{goodman2007,
  title = {Stopping at {{Nothing}}? Some {{Dilemmas}} of {{Data Monitoring}} in {{Clinical Trials}}},
  shorttitle = {Stopping at {{Nothing}}?},
  author = {Goodman, Steven N.},
  year = {2007},
  month = jun,
  journal = {Annals of Internal Medicine},
  volume = {146},
  number = {12},
  pages = {882},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-146-12-200706190-00010},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/H3JWNHJZ/Goodman - 2007 - Stopping at Nothing Some Dilemmas of Data Monitor.pdf}
}

@article{goodman2019,
  title = {Why Is {{Getting Rid}} of {{{\emph{P}}}} -{{Values So Hard}}? Musings on {{Science}} and {{Statistics}}},
  shorttitle = {Why Is {{Getting Rid}} of {{{\emph{P}}}} -{{Values So Hard}}?},
  author = {Goodman, Steven N.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {26--30},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1558111},
  abstract = {The current concerns about reproducibility have focused attention on proper use of statistics across the sciences. This gives statisticians an extraordinary opportunity to change what are widely regarded as statistical practices detrimental to the cause of good science. However, how that should be done is enormously complex, made more difficult by the balkanization of research methods and statistical traditions across scientific subdisciplines. Working within those sciences while also allying with science reform movements\textemdash operating simultaneously on the micro and macro levels\textemdash are the key to making lasting change in applied science.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/72TPSM99/Goodman - 2019 - Why is Getting Rid of P -Values So Hard Mu.pdf}
}

@article{hornsey2017,
  title = {Attitude Roots and {{Jiu Jitsu}} Persuasion: Understanding and Overcoming the Motivated Rejection of Science},
  shorttitle = {Attitude Roots and {{Jiu Jitsu}} Persuasion},
  author = {Hornsey, Matthew J. and Fielding, Kelly S.},
  year = {2017},
  journal = {The American Psychologist},
  volume = {72},
  number = {5},
  pages = {459--473},
  issn = {1935-990X},
  doi = {10.1037/a0040437},
  abstract = {There is a worryingly large chasm between scientific consensus and popular opinion. Roughly one third of Americans are skeptical that humans are primarily responsible for climate change; rates of some infectious diseases are climbing in the face of anti-immunization beliefs; and significant numbers of the population worldwide are antievolution creationists. It is easy to assume that resistance to an evidence-based message is a result of ignorance or failure to grasp evidence (the "deficit model" of science communication). But increasingly, theorists understand there are limits to this approach, and that if people are motivated to reject science, then repeating evidence will have little impact. In an effort to create a transtheoretical language for describing these underlying motivations, we introduce the notion of "attitude roots." Attitude roots are the underlying fears, ideologies, worldviews, and identity needs that sustain and motivate specific "surface" attitudes like climate skepticism and creationism. It is the antiscience attitude that people hear and see, but it is the attitude root-what lies under the surface-that allows the surface attitudes to survive even when they are challenged by evidence. We group these attitude roots within 6 themes-worldviews, conspiratorial ideation, vested interests, personal identity expression, social identity needs, and fears and phobias-and review literature relevant to them. We then use these insights to develop a "jiu jitsu" model of persuasion that places emphasis on creating change by aligning with (rather than competing with) these attitude roots. (PsycINFO Database Record},
  langid = {english},
  pmid = {28726454},
  keywords = {Attitude,Climate Change,Comprehension,Humans,Motivation,Persuasive Communication,Rejection; Psychology,Science}
}

@incollection{hornstein1988,
  title = {Quantifying Psychological Phenomena:  Debates, Dilemmas, and Implications},
  shorttitle = {Quantifying Psychological Phenomena},
  booktitle = {The Rise of Experimentation in {{American}} Psychology},
  author = {Hornstein, Gail A.},
  year = {1988},
  pages = {1--34},
  publisher = {{Yale University Press}},
  address = {{New Haven, CT, US}},
  abstract = {analyze the complex social and historical processes underlying the movement toward a quantitative perspective, as well as the means by which the move was accomplished, the functions it served, and the consequences it has had for the nature of research practice in psychology  difficulties that faced psychologists as they attempted to quantify psychological phenomena, and the repeated reformulations, debates, and negotiations that took place regarding the conceptualization of quantification and the general possibility of mental measurement  psychophysics / mental testing: the quantification of intelligence (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-300-04153-8},
  keywords = {Experimentation,History,Intelligence Measures,Psychophysics,Social Processes},
  file = {/Users/shilaan/Zotero/storage/SDHVGGDD/1988-98089-001.html}
}

@article{james1892,
  title = {A {{Plea}} for {{Psychology}} as a '{{Natural Science}}'},
  author = {James, William},
  year = {1892},
  journal = {The Philosophical Review},
  volume = {1},
  number = {2},
  pages = {146--153},
  publisher = {{[Duke University Press, Philosophical Review]}},
  issn = {0031-8108},
  doi = {10.2307/2175743}
}

@book{jennison2000,
  title = {Group Sequential Methods with Applications to Clinical Trials},
  author = {Jennison, Christopher and Turnbull, Bruce W},
  year = {2000},
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Group sequential methods answer the needs of clinical trial monitoring committees who must assess the data available at an interim analysis. These interim results may provide grounds for terminating the study-effectively reducing costs-or may benefit the general patient population by allowing early dissemination of its findings. Group sequential methods provide a means to balance the ethical and financial advantages of stopping a study early against the risk of an incorrect conclusion.Group Sequential Methods with Applications to Clinical Trials describes group sequential stopping rules designed to reduce average study length and control Type I and II error probabilities. The authors present one-sided and two-sided tests, introduce several families of group sequential tests, and explain how to choose the most appropriate test and interim analysis schedule. Their topics include placebo-controlled randomized trials, bio-equivalence testing, crossover and longitudinal studies, and linear and generalized linear models.Research in group sequential analysis has progressed rapidly over the past 20 years. Group Sequential Methods with Applications to Clinical Trials surveys and extends current methods for planning and conducting interim analyses. It provides straightforward descriptions of group sequential hypothesis tests in a form suited for direct application to a wide variety of clinical trials. Medical statisticians engaged in any investigations planned with interim analyses will find this book a useful and important tool.},
  isbn = {978-0-8493-0316-6 978-1-58488-858-1 978-0-367-80532-6},
  langid = {english},
  annotation = {OCLC: 123299028}
}

@article{jick1979,
  title = {Mixing {{Qualitative}} and {{Quantitative Methods}}: Triangulation in {{Action}}},
  shorttitle = {Mixing {{Qualitative}} and {{Quantitative Methods}}},
  author = {Jick, Todd D.},
  year = {1979},
  journal = {Administrative Science Quarterly},
  volume = {24},
  number = {4},
  pages = {602--611},
  publisher = {{[Sage Publications, Inc., Johnson Graduate School of Management, Cornell University]}},
  issn = {0001-8392},
  doi = {10.2307/2392366},
  file = {/Users/shilaan/Zotero/storage/DGHZ84WB/Jick - 1979 - Mixing Qualitative and Quantitative Methods Trian.pdf}
}

@book{kant1786,
  title = {Kant: Metaphysical {{Foundations}} of {{Natural Science}}},
  shorttitle = {Kant},
  author = {Kant, Immanuel},
  editor = {Friedman, Michael},
  translator = {Friedman, Michael},
  year = {1786},
  series = {Cambridge {{Texts}} in the {{History}} of {{Philosophy}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511809613},
  abstract = {Kant was centrally concerned with issues in the philosophy of natural science throughout his career. The Metaphysical Foundations of Natural Science presents his most mature reflections on these themes in the context of both his 'critical' philosophy, presented in the Critique of Pure Reason, and the natural science of his time. This volume presents a translation by Michael Friedman which is especially clear and accurate. There are explanatory notes indicating some of the main connections between the argument of the Metaphysical Foundations and the first Critique - as well as parallel connections to Newton's Principia. The volume is completed by an historical and philosophical introduction and a guide to further reading.},
  isbn = {978-0-521-83616-6},
  file = {/Users/shilaan/Zotero/storage/BL4SDFG8/C1B1176398A869C4C5A6B822535CEDFB.html}
}

@article{kruschke2018,
  title = {The {{Bayesian New Statistics}}: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {Kruschke, John K. and Liddell, Torrin M.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {178--206},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed ``the New Statistics'' (Cumming, 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/L5N322QY/Kruschke and Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf}
}

@article{lakens2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: Sequential Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  month = dec,
  journal = {European Journal of Social Psychology},
  volume = {44},
  number = {7},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/5J354TXG/Lakens - 2014 - Performing high-powered studies efficiently with s.pdf}
}

@article{lakens2017,
  title = {Power {{Analysis}} and {{Effect Size}}},
  author = {Lakens, Daniel and Biesanz, Jeremy},
  year = {2017},
  journal = {Society for the Improvement of Psychological Science Meeting},
  publisher = {{OSF}},
  doi = {None},
  abstract = {Workshop materials, slides, examples, and R code      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/GD55H3MZ/kdyfc.html}
}

@article{lakens2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: A {{Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/AWIPR9N4/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf}
}

@article{lakens2018a,
  title = {Justify Your Alpha},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {168--171},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  copyright = {2018 The Publisher},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/2LRRLTB4/Lakens et al. - 2018 - Justify your alpha.pdf;/Users/shilaan/Zotero/storage/IT6PDA3K/s41562-018-0311-x.html}
}

@misc{lakens2020,
  title = {Statistical {{Inferences}}: Sequential {{Analysis}}},
  author = {Lakens, Dani{\"e}l},
  year = {2020},
  journal = {GitHub},
  abstract = {Contribute to Lakens/statistical\_inferences development by creating an account on GitHub.},
  howpublished = {https://github.com/Lakens/statistical\_inferences},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/C949P845/14-sequential.html}
}

@techreport{lakens2020a,
  title = {The Practical Alternative to the P-Value Is the Correctly Used p-Value},
  author = {Lakens, Daniel},
  year = {2020},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/shm8v},
  abstract = {Due to the strong overreliance on p-values in the scientific literature some researchers have argued that p-values should be abandoned or banned, and that we need to move beyond p-values and embrace practical alternatives. When proposing alternatives to p-values statisticians often commit the `Statistician's Fallacy', where they declare which statistic researchers really `want to know'. Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p-value. As long as null-hypothesis tests have been criticized, researchers have suggested to include minimum-effect tests and equivalence tests in our statistical toolbox, and these tests (even though they return p-values) have the potential to greatly improve the questions researchers ask. It is clear there is room for improvement in how we teach p-values. If anyone really believes p-values are an important cause of problems in science, preventing the misinterpretation of p-values by developing better evidence-based education and user-centered statistical software should be a top priority. Telling researchers which statistic they should use has distracted us from examining more important questions, such as asking researchers what they want to know when they do scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
  keywords = {Meta-science},
  file = {/Users/shilaan/Zotero/storage/7UZQCVQ9/Lakens - 2019 - The practical alternative to the p-value is the co.pdf}
}

@article{lan1983,
  title = {Discrete Sequential Boundaries for Clinical Trials},
  author = {Lan, K. K. Gordan and Demets, David L.},
  year = {1983},
  month = dec,
  journal = {Biometrika},
  volume = {70},
  number = {3},
  pages = {659--663},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/70.3.659},
  abstract = {AbstractSUMMARY. Pocock (1977), O'Brien \&amp; Fleming (1979) and Slud \&amp; Wei (1982) have proposed different methods to construct discrete sequential boundari},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/YSXIDK5V/Gordon Lan and Demets - 1983 - Discrete sequential boundaries for clinical trials.pdf;/Users/shilaan/Zotero/storage/F39XWL3I/247777.html;/Users/shilaan/Zotero/storage/UY4QCVQ9/247777.html}
}

@article{landy20200116,
  title = {Crowdsourcing Hypothesis Tests: Making Transparent How Design Choices Shape Research Results.},
  shorttitle = {Crowdsourcing Hypothesis Tests},
  author = {Landy, Justin F. and Jia, Miaolei (Liam) and Ding, Isabel L. and Viganola, Domenico and Tierney, Warren and Dreber, Anna and Johannesson, Magnus and Pfeiffer, Thomas and Ebersole, Charles R. and Gronau, Quentin F. and Ly, Alexander and {van den Bergh}, Don and Marsman, Maarten and Derks, Koen and Wagenmakers, Eric-Jan and Proctor, Andrew and Bartels, Daniel M. and Bauman, Christopher W. and Brady, William J. and Cheung, Felix and Cimpian, Andrei and Dohle, Simone and Donnellan, M. Brent and Hahn, Adam and Hall, Michael P. and {Jim{\'e}nez-Leal}, William and Johnson, David J. and Lucas, Richard E. and Monin, Beno{\^i}t and Montealegre, Andres and Mullen, Elizabeth and Pang, Jun and Ray, Jennifer and Reinero, Diego A. and Reynolds, Jesse and Sowden, Walter and Storage, Daniel and Su, Runkun and Tworek, Christina M. and Van Bavel, Jay J. and Walco, Daniel and Wills, Julian and Xu, Xiaobing and Yam, Kai Chi and Yang, Xiaoyu and Cunningham, William A. and Schweinsberg, Martin and Urwitz, Molly and Collaboration, The Crowdsourcing Hypothesis Tests and Uhlmann, Eric L.},
  year = {20200116},
  journal = {Psychological Bulletin},
  volume = {146},
  number = {5},
  pages = {451},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1455},
  doi = {10.1037/bul0000220},
  file = {/Users/shilaan/Zotero/storage/K5HZA86N/Landy et al. - Crowdsourcing hypothesis tests Making transparent.pdf;/Users/shilaan/Zotero/storage/9LZBN44H/2020-02973-001.html}
}

@misc{lawrencelivermorenationallaboratory2016,
  title = {All {{About}} That {{Bayes}}: Probability, {{Statistics}}, and the {{Quest}} to {{Quantify Uncertainty}}},
  shorttitle = {All {{About}} That {{Bayes}}},
  author = {{Lawrence Livermore National Laboratory}},
  year = {2016},
  month = sep,
  abstract = {Lawrence Livermore National Laboratory statistician Kristin Lennox delves into the history of statistics and probability in this talk, "All About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty," given at LLNL on July 28, 2016.  Abstract: The great Bayesian vs. Frequentist war has raged within statistics for almost 100 years, much to the confusion of outsiders. The Bayesian/Frequentist question is no longer academic, with both styles of inference appearing frequently in scientific literature and even the news. In this talk, Kristin Lennox aims to explain the great divide to non-statisticians, and also to answer the most important statistical question of all: how does probability allow us to better understand our world? View the PowerPoint slides from the talk at http://www.slideshare.net/LivermoreLa...}
}

@article{leary1987,
  title = {Telling Likely Stories: The Rhetoric of the {{New Psychology}}, 1880-1920},
  shorttitle = {Telling Likely Stories},
  author = {Leary, David E.},
  year = {1987},
  journal = {Journal of the History of the Behavioral Sciences},
  volume = {23},
  number = {4},
  pages = {315--331},
  issn = {1520-6696},
  doi = {10.1002/1520-6696(198710)23:4<315::AID-JHBS2300230402>3.0.CO;2-V},
  abstract = {This is a story about the New Psychologists who strove at the turn of the century to institutionalize a new science and to create a new set of professional roles. More particularly, it is about the rhetorical fabric they wove around the nascent science of psychology. The article focuses, one by one, on different strands of this fabric \textendash{} on (1) what persuaded the first generation of American psychologists to take an interest in the New Psychology; (2) the arguments these aspiring psychologists presented to presidents and trustees to insure that they could pursue their interest within particular institutional settings; (3) the arguments they put forth against the rights of other persons to engage in similar, competing pursuits; (4) the arguments they laid before various administrators, officials, interest groups, and the general public to guarantee continued and even increased support; and (5) the arguments they presented in the form of theories and practices developed between approximately 1880 and 1920. In this way, it attempts to construct a likely story about the establishment of the New Psychology in America.1},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1520-6696\%28198710\%2923\%3A4\%3C315\%3A\%3AAID-JHBS2300230402\%3E3.0.CO\%3B2-V},
  file = {/Users/shilaan/Zotero/storage/FLLYSCMR/Leary - 1987 - Telling likely stories The rhetoric of the New Ps.pdf;/Users/shilaan/Zotero/storage/ABAT2CJI/1520-6696(198710)234315AID-JHBS23002304023.0.html}
}

@article{mayo2016,
  title = {Don't {{Throw Out}} the {{Error Control Baby With}} the {{Bad Statistics Bathwater}}: A {{Commentary}}},
  author = {Mayo, Deborah G},
  year = {2016},
  journal = {The American Statistician},
  volume = {70},
  number = {Online Discussion},
  pages = {1--2},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/M3VBJGV5/Mayo - Don‚Äôt Throw Out the Error Control Baby With the Ba.pdf}
}

@book{mayo2018,
  title = {Statistical {{Inference}} as {{Severe Testing}}: How to {{Get Beyond}} the {{Statistics Wars}}},
  shorttitle = {Statistical {{Inference}} as {{Severe Testing}}},
  author = {Mayo, Deborah G.},
  year = {2018},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781107286184},
  abstract = {Mounting failures of replication in social and biological sciences give a new urgency to critically appraising proposed reforms. This book pulls back the cover on disagreements between experts charged with restoring integrity to science. It denies two pervasive views of the role of probability in inference: to assign degrees of belief, and to control error rates in a long run. If statistical consumers are unaware of assumptions behind rival evidence reforms, they can't scrutinize the consequences that affect them (in personalized medicine, psychology, etc.). The book sets sail with a simple tool: if little has been done to rule out flaws in inferring a claim, then it has not passed a severe test. Many methods advocated by data experts do not stand up to severe scrutiny and are in tension with successful strategies for blocking or accounting for cherry picking and selective reporting. Through a series of excursions and exhibits, the philosophy and history of inductive inference come alive. Philosophical tools are put to work to solve problems about science and pseudoscience, induction and falsification.},
  isbn = {978-1-107-05413-4},
  file = {/Users/shilaan/Zotero/storage/5KX7Z45D/D9DF409EF568090F3F60407FF2B973B2.html}
}

@article{mcgrath1981,
  title = {Dilemmatics: The {{Study}} of {{Research Choices}} and {{Dilemmas}}},
  shorttitle = {Dilemmatics},
  author = {McGrath, Joseph E.},
  year = {1981},
  month = nov,
  journal = {American Behavioral Scientist},
  volume = {25},
  number = {2},
  pages = {179--210},
  issn = {0002-7642, 1552-3381},
  doi = {10.1177/000276428102500205},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/H7BUE6ZP/McGrath - 1981 - Dilemmatics The Study of Research Choices and Dil.pdf}
}

@article{miller2016,
  title = {Changing {{Norms}} to {{Change Behavior}}},
  author = {Miller, Dale T. and Prentice, Deborah A.},
  year = {2016},
  journal = {Annual Review of Psychology},
  volume = {67},
  number = {1},
  pages = {339--361},
  doi = {10.1146/annurev-psych-010814-015013},
  abstract = {Providing people with information about the behavior and attitudes of their peers is a strategy commonly employed by those seeking to reduce behavior deemed harmful either to individuals (e.g., high alcohol consumption) or the collective (e.g., high energy consumption). We review norm-based interventions, detailing the logic behind them and the various forms they can take. We give special attention to interventions designed to decrease college students' drinking and increase environment-friendly behaviors. We identify the conditions under which norm information has the highest likelihood of changing the targeted behavior and discuss why this is the case.},
  pmid = {26253542},
  keywords = {drinking behavior,energy conservation,interventions,social norm marketing,social norms},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-psych-010814-015013}
}

@article{miller2020,
  title = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing: The Independent Segments Procedure.},
  shorttitle = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2020},
  month = oct,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000350},
  abstract = {We propose a new sequential hypothesis testing procedure in which data are collected and analyzed in a series of independent segments. As in fixed-sample hypothesis testing and in previous sequential procedures, the overall \textvisiblespace{} level can be set to any desired value. Like other sequential procedures, the independent segments procedure generally requires smaller samples than fixed-sample procedures\textemdash{} often approximately 30\% smaller\textemdash to achieve the same \textvisiblespace{} level and statistical power. Relative to other sequential procedures, the new method has the advantages that it is simpler to use, requires fewer assumptions, and can be used with a wider array of statistical tests. Thus, in some circumstances the independent segments procedure may provide an attractive option for increasing the efficiency of statistical testing.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/ABRC9QJY/Miller and Ulrich - 2020 - A simple, general, and efficient method for sequen.pdf}
}

@article{morey2011,
  title = {Bayes Factor Approaches for Testing Interval Null Hypotheses.},
  author = {Morey, Richard D. and Rouder, Jeffrey N.},
  year = {2011},
  month = dec,
  journal = {Psychological Methods},
  volume = {16},
  number = {4},
  pages = {406--419},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0024377},
  abstract = {Psychological theories are statements of constraint. The role of hypothesis testing in psychology is to test whether specific theoretical constraints hold in data. Bayesian statistics is well suited to the task of finding supporting evidence for constraint, because it allows for comparing evidence for 2 hypotheses against each another. One issue in hypothesis testing is that constraints may hold only approximately rather than exactly, and the reason for small deviations may be trivial or uninteresting. In the large-sample limit, these uninteresting, small deviations lead to the rejection of a useful constraint. In this article, we develop several Bayes factor 1-sample tests for the assessment of approximate equality and ordinal constraints. In these tests, the null hypothesis covers a small interval of non-0 but negligible effect sizes around 0. These Bayes factors are alternatives to previously developed Bayes factors, which do not allow for interval null hypotheses, and may especially prove useful to researchers who use statistical equivalence testing. To facilitate adoption of these Bayes factor tests, we provide easy-to-use software.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/9T2ZNVKY/Morey and Rouder - 2011 - Bayes factor approaches for testing interval null .pdf}
}

@article{morey2016,
  title = {Why Most of Psychology Is Statistically Unfalsifiable ({{Version}} 1.0)},
  author = {Morey, Richard D and Lakens, Dani{\"e}l},
  year = {2016},
  doi = {10.5281/zenodo.838685},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/F255J25L/Morey and Lakens - Why most of psychology is statistically unfalsiÔ¨Åab.pdf}
}

@article{moshontz2018,
  title = {The {{Psychological Science Accelerator}}: Advancing {{Psychology Through}} a {{Distributed Collaborative Network}}},
  shorttitle = {The {{Psychological Science Accelerator}}},
  author = {Moshontz, Hannah and Campbell, Lorne and Ebersole, Charles R. and IJzerman, Hans and Urry, Heather L. and Forscher, Patrick S. and Grahe, Jon E. and McCarthy, Randy J. and Musser, Erica D. and Antfolk, Jan and Castille, Christopher M. and Evans, Thomas Rhys and Fiedler, Susann and Flake, Jessica Kay and Forero, Diego A. and Janssen, Steve M. J. and Keene, Justin Robert and Protzko, John and Aczel, Balazs and {\'A}lvarez Solas, Sara and Ansari, Daniel and Awlia, Dana and Baskin, Ernest and Batres, Carlota and {Borras-Guevara}, Martha Lucia and Brick, Cameron and Chandel, Priyanka and Chatard, Armand and Chopik, William J. and Clarance, David and Coles, Nicholas A. and Corker, Katherine S. and Dixson, Barnaby James Wyld and Dranseika, Vilius and Dunham, Yarrow and Fox, Nicholas W. and Gardiner, Gwendolyn and Garrison, S. Mason and Gill, Tripat and Hahn, Amanda C. and Jaeger, Bastian and Ka{\v c}m{\'a}r, Pavol and Kaminski, Gwena{\"e}l and Kanske, Philipp and Kekecs, Zoltan and Kline, Melissa and Koehn, Monica A. and Kujur, Pratibha and Levitan, Carmel A. and Miller, Jeremy K. and Okan, Ceylan and Olsen, Jerome and {Oviedo-Trespalacios}, Oscar and {\"O}zdo{\u g}ru, Asil Ali and Pande, Babita and Parganiha, Arti and Parveen, Noorshama and Pfuhl, Gerit and Pradhan, Sraddha and Ropovik, Ivan and Rule, Nicholas O. and Saunders, Blair and Schei, Vidar and Schmidt, Kathleen and Singh, Margaret Messiah and Sirota, Miroslav and Steltenpohl, Crystal N. and Stieger, Stefan and Storage, Daniel and Sullivan, Gavin Brent and Szabelska, Anna and Tamnes, Christian K. and Vadillo, Miguel A. and Valentova, Jaroslava V. and Vanpaemel, Wolf and Varella, Marco A. C. and Vergauwe, Evie and Verschoor, Mark and Vianello, Michelangelo and Voracek, Martin and Williams, Glenn P. and Wilson, John Paul and Zickfeld, Janis H. and Arnal, Jack D. and Aydin, Burak and Chen, Sau-Chin and DeBruine, Lisa M. and Fernandez, Ana Maria and Horstmann, Kai T. and Isager, Peder M. and Jones, Benedict and Kapucu, Aycan and Lin, Hause and Mensink, Michael C. and Navarrete, Gorka and Silan, Miguel A. and Chartier, Christopher R.},
  year = {2018},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {501--515},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918797607},
  abstract = {Concerns about the veracity of psychological research have been growing. Many findings in psychological science are based on studies with insufficient statistical power and nonrepresentative samples, or may otherwise be limited to specific, ungeneralizable settings or populations. Crowdsourced research, a type of large-scale collaboration in which one or more research projects are conducted across multiple lab sites, offers a pragmatic solution to these and other current methodological challenges. The Psychological Science Accelerator (PSA) is a distributed network of laboratories designed to enable and support crowdsourced research projects. These projects can focus on novel research questions or replicate prior research in large, diverse samples. The PSA's mission is to accelerate the accumulation of reliable and generalizable evidence in psychological science. Here, we describe the background, structure, principles, procedures, benefits, and challenges of the PSA. In contrast to other crowdsourced research networks, the PSA is ongoing (as opposed to time limited), efficient (in that structures and principles are reused for different projects), decentralized, diverse (in both subjects and researchers), and inclusive (of proposals, contributions, and other relevant input from anyone inside or outside the network). The PSA and other approaches to crowdsourced psychological science will advance understanding of mental processes and behaviors by enabling rigorous research and systematic examination of its generalizability.},
  langid = {english},
  keywords = {crowdsourcing,generalizability,large-scale collaboration,Psychological Science Accelerator,theory development},
  file = {/Users/shilaan/Zotero/storage/CCB3LMU2/Moshontz et al. - 2018 - The Psychological Science Accelerator Advancing P.pdf}
}

@article{munoz2018,
  title = {We {{Ran}} 9 {{Billion Regressions}}: Eliminating {{False Positives}} through {{Computational Model Robustness}}},
  shorttitle = {We {{Ran}} 9 {{Billion Regressions}}},
  author = {Mu{\~n}oz, John and Young, Cristobal},
  year = {2018},
  month = aug,
  journal = {Sociological Methodology},
  volume = {48},
  number = {1},
  pages = {1--33},
  publisher = {{SAGE Publications Inc}},
  issn = {0081-1750},
  doi = {10.1177/0081175018777988},
  abstract = {False positive findings are a growing problem in many research literatures. We argue that excessive false positives often stem from model uncertainty. There are many plausible ways of specifying a regression model, but researchers typically report only a few preferred estimates. This raises the concern that such research reveals only a small fraction of the possible results and may easily lead to nonrobust, false positive conclusions. It is often unclear how much the results are driven by model specification and how much the results would change if a different plausible model were used. Computational model robustness analysis addresses this challenge by estimating all possible models from a theoretically informed model space. We use large-scale random noise simulations to show (1) the problem of excess false positive errors under model uncertainty and (2) that computational robustness analysis can identify and eliminate false positives caused by model uncertainty. We also draw on a series of empirical applications to further illustrate issues of model uncertainty and estimate instability. Computational robustness analysis offers a method for relaxing modeling assumptions and improving the transparency of applied research.},
  file = {/Users/shilaan/Zotero/storage/2N5PLNYL/Mu√±oz and Young - 2018 - We Ran 9 Billion Regressions Eliminating False Po.pdf}
}

@article{nelson2018,
  title = {Psychology's {{Renaissance}}},
  author = {Nelson, Leif D. and Simmons, Joseph and Simonsohn, Uri},
  year = {2018},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {69},
  number = {1},
  pages = {511--534},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122216-011836},
  abstract = {In 2010\textendash 2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology's Renaissance. We begin by describing how psychologists' concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/AWYVDDGF/Nelson et al. - 2018 - Psychology's Renaissance.pdf}
}

@article{nelson2018a,
  title = {Psychology's {{Renaissance}}},
  author = {Nelson, Leif D. and Simmons, Joseph and Simonsohn, Uri},
  year = {2018},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {69},
  number = {1},
  pages = {511--534},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122216-011836},
  abstract = {In 2010\textendash 2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology's Renaissance. We begin by describing how psychologists' concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/VRJN84RH/Nelson et al. - 2018 - Psychology's Renaissance.pdf}
}

@article{neyman1933,
  title = {On the {{Problem}} of the {{Most Efficient Tests}} of {{Statistical Hypotheses}}},
  author = {Neyman, J. and Pearson, E. S.},
  year = {1933},
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {231},
  pages = {289--337},
  publisher = {{The Royal Society}},
  issn = {0264-3952}
}

@article{neyman1957,
  title = {"{{Inductive Behavior}}" as a {{Basic Concept}} of {{Philosophy}} of {{Science}}},
  author = {Neyman, J.},
  year = {1957},
  journal = {Revue de l'Institut International de Statistique / Review of the International Statistical Institute},
  volume = {25},
  number = {1/3},
  pages = {7--22},
  publisher = {{[International Statistical Institute (ISI), Wiley]}},
  issn = {0373-1138},
  doi = {10.2307/1401671},
  abstract = {Le sujet trait\'e se rapporte \`a l'ultime phase de la recherche scientifique, la phase qu'on d\'esigne souvent par le terme "raisonnement inductif". Une analyse des processus mentaux qui accompagnent cette phase indique qu'ils se r\'eduisent \`a une combinaison des \'el\'ements suivants: (i) un effort d'imagination et de m\'emoire pour passer en revue les hypoth\`eses possibles qui puissent expliquer les ph\'enom\`enes observ\'es; (ii) les d\'eductions de ces hypoth\`eses et (iii) un acte de volont\'e d'agir conform\'ement \`a une des hypoth\`eses consid\'er\'ees. Par cons\'equent, le terme 'raisonnement inductif" semble d\'eplac\'e et le terme "comportement inductif" para\^it pr\'ef\'erable pour d\'esigner la phase (iii). Les efforts faits pour pr\'eciser le concept de raisonnement inductif conduisent aux formules dogmatiques d\'esign\'ees pour prescrire universellement les degr\'es de confiance qu'un chercheur devrait attribuer aux diff\'erentes hypoth\`eses consid\'er\'ees. A cause du caract\`ere subjectif des degr\'es de confiance, les tentatives d'\'etablir ces formules comme des normes universelles des attitudes, conduisent \`a des controverses anim\'ees. D'autre part, si l'on se rend compte du fait que la suite n\'ecessaire d'une exp\'erience donn\'ee est une action de la part du chercheur, on voit imm\'ediatement que ces disputes sont inutiles. L'avantage de chaque action consid\'er\'ee d\'epend des attitudes individuelles du chercheur et est une fonction des circonstances inconnues, qu'on d\'esigne par le terme "\'etat de l'univers". Si le choix de l'action se fait par une m\'ethode d\'etermin\'ee utilisant les valeurs observ\'ees de quelques variables al\'eatoires dont la loi d\'epend de l'\'etat de l'univers, l'action choisie devient al\'eatoire ellem\^eme, ainsi que l'avantage gagn\'e par cette action. Le probl\`eme de la statistique math\'ematique, consid\'er\'ee comme la th\'eorie math\'ematique du comportement inductif, est de d\'eterminer les r\`egles d'ajuster notre comportement aux valeurs des variables al\'eatoires observ\'ees de telle mani\`ere que la loi de probabilit\'e de l'avantage \`a gagner soit "optimum". La d\'efinition de cet "optimum", ainsi que la d\'efinition de l'avantage d'une action en fonction de l'\'etat de l'univers, sont les circonstances nonstatistiques de la situation et restent \`a la disposition du chercheur, affect\'e par ses propres attitudes et croyances individuelles. La notion (mais pas le terme) du comportement inductif a \'et\'e connue \`a Laplace. Suivant Laplace, elle a \'et\'e adopt\'ee par Gauss. L'adoption du carr\'e de l'erreur d'une estimation statistique comme la mesure du d\'esavantage de cette estimation, et l'adoption de l'esp\'erance math\'ematique du d\'esavantage d'une estimation comme la mesure du d\'esavantage d'une m\'ethode d'estimation sans biais, ont amen\'e Gauss au d\'eveloppement de la m\'ethode des moindres carr\'es. Il semble probable que le traitement moderne des probl\`emes statistiques du point de vue du comportement inductif conduira \`a des d\'eveloppements futurs d'une importance comparable.},
  file = {/Users/shilaan/Zotero/storage/PIIVGPD4/Neyman - 1957 - Inductive Behavior as a Basic Concept of Philoso.pdf}
}

@article{nickerson2000,
  title = {Null Hypothesis Significance Testing: A Review of an Old and Continuing Controversy},
  shorttitle = {Null Hypothesis Significance Testing},
  author = {Nickerson, R. S.},
  year = {2000},
  month = jun,
  journal = {Psychological Methods},
  volume = {5},
  number = {2},
  pages = {241--301},
  issn = {1082-989X},
  doi = {10.1037/1082-989x.5.2.241},
  abstract = {Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data.},
  langid = {english},
  pmid = {10937333},
  keywords = {Data Interpretation; Statistical,Humans,Psychology; Experimental,Psychometrics,Reproducibility of Results},
  file = {/Users/shilaan/Zotero/storage/WPD7DH5N/Nickerson - 2000 - Null hypothesis significance testing a review of .pdf}
}

@article{nosek2012,
  title = {Scientific {{Utopia}}: {{II}}. {{Restructuring Incentives}} and {{Practices}} to {{Promote Truth Over Publishability}}},
  shorttitle = {Scientific {{Utopia}}},
  author = {Nosek, Brian A. and Spies, Jeffrey R. and Motyl, Matt},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {615--631},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691612459058},
  abstract = {An academic scientist?s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive?getting it right?competitive with the more tangible and concrete incentive?getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.},
  file = {/Users/shilaan/Zotero/storage/CXFQ8J7G/Nosek et al. - 2012 - Scientific Utopia II. Restructuring Incentives an.pdf}
}

@book{oakes1986,
  title = {Statistical {{Inference}}: A {{Commentary}} for the {{Social}} and {{Behavioural Sciences}}},
  shorttitle = {Statistical {{Inference}}},
  author = {Oakes, Michael},
  year = {1986},
  month = may,
  publisher = {{Wiley}},
  abstract = {Critical analysis of the use of statistical inference in social and behavioural research, its proper role, logic, and its abuse. Argues that ignorance and misunderstanding of the role of statistical inference has had a detrimental effect upon research in the field. Points the way to an appropriate appreciation of the part played by quantification in these disciplines. Examines the significance test versus interval estimation and evaluates Neyman-Pearson, Fisherian, Bayesian, and Likelihood inference. Presents arguments in a plainly written, easily understood manner.},
  googlebooks = {OhFHAAAAMAAJ},
  isbn = {978-0-471-10443-8},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Psychology / General}
}

@article{pashler2012,
  title = {Editors' {{Introduction}} to the {{Special Section}} on {{Replicability}} in {{Psychological Science}}: A {{Crisis}} of {{Confidence}}?},
  shorttitle = {Editors' {{Introduction}} to the {{Special Section}} on {{Replicability}} in {{Psychological Science}}},
  author = {Pashler, Harold and Wagenmakers, Eric{\textendash}Jan},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {528--530},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691612465253},
  langid = {english}
}

@article{perugini2014,
  title = {Safeguard {{Power}} as a {{Protection Against Imprecise Power Estimates}}},
  author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
  year = {2014},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {319--332},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614528519},
  abstract = {An essential first step in planning a confirmatory or a replication study is to determine the sample size necessary to draw statistically reliable inferences using power analysis. A key problem, however, is that what is available is the sample-size estimate of the effect size, and its use can lead to severely underpowered studies when the effect size is overestimated. As a potential remedy, we introduce safeguard power analysis, which uses the uncertainty in the estimate of the effect size to achieve a better likelihood of correctly identifying the population effect size. Using a lower-bound estimate of the effect size, in turn, allows researchers to calculate a sample size for a replication study that helps protect it from being underpowered. We show that in most common instances, compared with nominal power, safeguard power is higher whereas standard power is lower. We additionally recommend the use of safeguard power analysis to evaluate the strength of the evidence provided by the original study.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/VCFHDX8P/Perugini et al. - 2014 - Safeguard Power as a Protection Against Imprecise .pdf}
}

@article{pocock1977,
  title = {Group {{Sequential Methods}} in the {{Design}} and {{Analysis}} of {{Clinical Trials}}},
  author = {Pocock, Stuart J.},
  year = {1977},
  journal = {Biometrika},
  volume = {64},
  number = {2},
  pages = {191--199},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2335684},
  abstract = {In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes be statistically superior to standard sequential designs.},
  file = {/Users/shilaan/Zotero/storage/X53WL252/Pocock - 1977 - Group Sequential Methods in the Design and Analysi.pdf}
}

@article{pocock1989,
  title = {Practical Problems in Interim Analyses, with Particular Regard to Estimation},
  author = {Pocock, Stuart J. and Hughes, Michael D.},
  year = {1989},
  month = dec,
  journal = {Controlled Clinical Trials},
  volume = {10},
  number = {4},
  pages = {209--221},
  issn = {01972456},
  doi = {10.1016/0197-2456(89)90059-7},
  abstract = {This article considers some of the practical problems inherent in interim analyses and stopping rules for randomized clinical trials. Topics covered include group sequential designs, trials with unplanned interim analyses, estimation problems in clinical trials with planned interim analyses, and the balance between individual and collective ethics. Particular attention is paid to the fact that clinical trials that stop early are prone to exaggerate the magnitude of treatment effect. Accordingly, a Bayesian "shrinkage" method of analysis is proposed to help quantify the extent to which surprisingly large point and interval estimates of treatment difference in clinical trials that stop early should be moderated.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/B3NHCU7D/Pocock and Hughes - 1989 - Practical problems in interim analyses, with parti.pdf}
}

@book{porter2020,
  title = {Trust in {{Numbers}}: The {{Pursuit}} of {{Objectivity}} in {{Science}} and {{Public Life}}},
  shorttitle = {Trust in {{Numbers}}},
  author = {Porter, Theodore M.},
  year = {2020},
  publisher = {{Princeton University Press}},
  address = {{Princeton}},
  abstract = {A foundational work on historical and social studies of quantificationWhat accounts for the prestige of quantitative methods? The usual answer is that quantification is desirable in social investigation as a result of its successes in science. Trust in Numbers questions whether such success in the study of stars, molecules, or cells should be an attractive model for research on human societies, and examines why the natural sciences are highly quantitative in the first place. Theodore Porter argues that a better understanding of the attractions of quantification in business, government, and social research brings a fresh perspective to its role in psychology, physics, and medicine. Quantitative rigor is not inherent in science but arises from political and social pressures, and objectivity derives its impetus from cultural contexts. In a new preface, the author sheds light on the current infatuation with quantitative methods, particularly at the intersection of science and bureaucracy.},
  isbn = {978-0-691-21054-4}
}

@article{pramanik2021,
  title = {A Modified Sequential Probability Ratio Test},
  author = {Pramanik, Sandipan and Johnson, Valen E. and Bhattacharya, Anirban},
  year = {2021},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {101},
  pages = {102505},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2021.102505},
  abstract = {We describe a modified sequential probability ratio test that can be used to reduce the average sample size required to perform statistical hypothesis tests at specified levels of significance and power. Examples are provided for z tests, t tests, and tests of binomial success probabilities. A description of a software package to implement the test designs is provided. We compare the sample sizes required in fixed design tests conducted at 5\% significance levels to the average sample sizes required in sequential tests conducted at 0.5\% significance levels, and we find that the two sample sizes are approximately equal.},
  langid = {english},
  keywords = {Bayes factor,MaxSPRT,Sequential Bayes factor,Sequential design,Sequential probability ratio test,Significance test,Uniformly most powerful Bayesian test},
  file = {/Users/shilaan/Zotero/storage/CAJG3GMH/Pramanik et al. - 2021 - A modified sequential probability ratio test.pdf}
}

@book{proschan2006,
  title = {Statistical Monitoring of Clinical Trials: A Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, K. K. Gordan and Wittes, Janet Turk},
  year = {2006},
  series = {Statistics for Biology and Health},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-0-387-30059-7},
  langid = {english},
  lccn = {R853.C55 P76 2006},
  keywords = {Bayes Theorem,Clinical trials,Clinical Trials,Data Interpretation; Statistical,Drugs,Statistical methods,Statistics,statistics \& numerical data,Testing},
  annotation = {OCLC: ocm71228590},
  file = {/Users/shilaan/Zotero/storage/JA23E4VM/Proschan et al. - 2006 - Statistical monitoring of clinical trials a unifi.pdf;/Users/shilaan/Zotero/storage/XTV5PIAT/Neyman Pearson On the problem of the most efficient tests of statistical hypotheses.pdf}
}

@article{rohrer2021,
  title = {Putting the {{Self}} in {{Self}}-{{Correction}}: Findings {{From}} the {{Loss}}-of-{{Confidence Project}}},
  shorttitle = {Putting the {{Self}} in {{Self}}-{{Correction}}},
  author = {Rohrer, Julia M. and Tierney, Warren and Uhlmann, Eric L. and DeBruine, Lisa M. and Heyman, Tom and Jones, Benedict and Schmukle, Stefan C. and Silberzahn, Raphael and Will{\'e}n, Rebecca M. and Carlsson, Rickard and Lucas, Richard E. and Strand, Julia and Vazire, Simine and Witt, Jessica K. and Zentall, Thomas R. and Chabris, Christopher F. and Yarkoni, Tal},
  year = {2021},
  month = mar,
  journal = {Perspectives on Psychological Science},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691620964106},
  abstract = {Science is often perceived to be a self-correcting enterprise. In principle, the assessment of scientific claims is supposed to proceed in a cumulative fashion, with the reigning theories of the day progressively approximating truth more accurately over time. In practice, however, cumulative self-correction tends to proceed less efficiently than one might naively suppose. Far from evaluating new evidence dispassionately and infallibly, individual scientists often cling stubbornly to prior findings. Here we explore the dynamics of scientific self-correction at an individual rather than collective level. In 13 written statements, researchers from diverse branches of psychology share why and how they have lost confidence in one of their own published findings. We qualitatively characterize these disclosures and explore their implications. A cross-disciplinary survey suggests that such loss-of-confidence sentiments are surprisingly common among members of the broader scientific population yet rarely become part of the public record. We argue that removing barriers to self-correction at the individual level is imperative if the scientific community as a whole is to achieve the ideal of efficient self-correction.},
  langid = {english},
  keywords = {incentive structure,knowledge accumulation,metascience,scientific errors,scientific falsification,self-correction},
  file = {/Users/shilaan/Zotero/storage/S7DXS2YA/Rohrer et al. - 2021 - Putting the Self in Self-Correction Findings From.pdf}
}

@article{romanyshyn1971,
  title = {Method and {{Meaning}} in {{Psychology}}: The {{Method}} Has Been the {{Message}}},
  shorttitle = {Method and {{Meaning}} in {{Psychology}}},
  author = {Romanyshyn, Robert D.},
  year = 1971,
  journal = {Journal of Phenomenological Psychology},
  volume = {2},
  number = {1},
  pages = {93--113},
  publisher = {{Brill Academic Publishers}},
  address = {{Leiden, Netherlands}},
  issn = {0047-2662},
  langid = {english},
  keywords = {Philosophy},
  file = {/Users/shilaan/Zotero/storage/RNX289SD/Romanyshyn - 1971 - Method and Meaning in Psychology The Method has b.pdf}
}

@article{rouder2016,
  title = {Is {{There}} a {{Free Lunch}} in {{Inference}}?},
  author = {Rouder, Jeffrey N. and Morey, Richard D. and Verhagen, Josine and Province, Jordan M. and Wagenmakers, Eric-Jan},
  year = {2016},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {8},
  number = {3},
  pages = {520--547},
  issn = {17568757},
  doi = {10.1111/tops.12214},
  abstract = {The field of psychology, including cognitive science, is vexed by a crisis of confidence. Although the causes and solutions are varied, we focus here on a common logical problem in inference. The default mode of inference is significance testing, which has a free lunch property where researchers need not make detailed assumptions about the alternative to test the null hypothesis. We present the argument that there is no free lunch; that is, valid testing requires that researchers test the null against a well-specified alternative. We show how this requirement follows from the basic tenets of conventional and Bayesian probability. Moreover, we show in both the conventional and Bayesian framework that not specifying the alternative may lead to rejections of the null hypothesis with scant evidence. We review both frequentist and Bayesian approaches to specifying alternatives, and we show how such specifications improve inference. The field of cognitive science will benefit because consideration of reasonable alternatives will undoubtedly sharpen the intellectual underpinnings of research.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/NHDCCU9I/Rouder et al. - 2016 - Is There a Free Lunch in Inference.pdf}
}

@article{rouder2016a,
  title = {Is {{There}} a {{Free Lunch}} in {{Inference}}?},
  author = {Rouder, Jeffrey N. and Morey, Richard D. and Verhagen, Josine and Province, Jordan M. and Wagenmakers, Eric-Jan},
  year = {2016},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {8},
  number = {3},
  pages = {520--547},
  issn = {17568757},
  doi = {10.1111/tops.12214},
  abstract = {The field of psychology, including cognitive science, is vexed by a crisis of confidence. Although the causes and solutions are varied, we focus here on a common logical problem in inference. The default mode of inference is significance testing, which has a free lunch property where researchers need not make detailed assumptions about the alternative to test the null hypothesis. We present the argument that there is no free lunch; that is, valid testing requires that researchers test the null against a well-specified alternative. We show how this requirement follows from the basic tenets of conventional and Bayesian probability. Moreover, we show in both the conventional and Bayesian framework that not specifying the alternative may lead to rejections of the null hypothesis with scant evidence. We review both frequentist and Bayesian approaches to specifying alternatives, and we show how such specifications improve inference. The field of cognitive science will benefit because consideration of reasonable alternatives will undoubtedly sharpen the intellectual underpinnings of research.},
  langid = {english}
}

@article{rozeboom1960,
  title = {The Fallacy of the Null-Hypothesis Significance Test},
  author = {Rozeboom, William W.},
  year = {1960},
  journal = {Psychological Bulletin},
  volume = {57},
  number = {5},
  pages = {416--428},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/h0042040},
  abstract = {Though several serious objections to the null-hypothesis significance test method are raised, "its most basic error lies in mistaking the aim of a scientific investigation to be a decision, rather than a cognitive evaluation\ldots{} . It is further argued that the proper application of statistics to scientific inference is irrevocably committed to extensive consideration of inverse probabilities, and to further this end, certain suggestions are offered." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Evaluation,Null Hypothesis Testing,Statistics,Theory Verification},
  file = {/Users/shilaan/Zotero/storage/F4484J64/Rozeboom - 1960 - The fallacy of the null-hypothesis significance te.pdf;/Users/shilaan/Zotero/storage/4SZJANVD/1961-04282-001.html;/Users/shilaan/Zotero/storage/M7H8J8ZB/1961-04282-001.html}
}

@article{schnuerch2020,
  title = {Controlling Decision Errors with Minimal Costs: The Sequential Probability Ratio t Test.},
  shorttitle = {Controlling Decision Errors with Minimal Costs},
  author = {Schnuerch, Martin and Erdfelder, Edgar},
  year = {2020},
  month = apr,
  journal = {Psychological Methods},
  volume = {25},
  number = {2},
  pages = {206--226},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000234},
  abstract = {For several years, the public debate in psychological science has been dominated by what is referred to as the reproducibility crisis. This crisis has, inter alia, drawn attention to the need for proper control of statistical decision errors in testing psychological hypotheses. However, conventional methods of error probability control often require fairly large samples. Sequential statistical tests provide an attractive alternative: They can be applied repeatedly during the sampling process and terminate whenever there is sufficient evidence in the data for one of the hypotheses of interest. Thus, sequential tests may substantially reduce the required sample size without compromising predefined error probabilities. Herein, we discuss the most efficient sequential design, the sequential probability ratio test (SPRT), and show how it is easily implemented for a 2-sample t test using standard statistical software. We demonstrate, by means of simulations, that the SPRT not only reliably controls error probabilities but also typically requires substantially smaller samples than standard t tests and other common sequential designs. Moreover, we investigate the robustness of the SPRT against violations of its assumptions. Finally, we illustrate the sequential t test by applying it to an empirical example and provide recommendations on how psychologists can employ it in their own research to benefit from its desirable properties.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/R2N7UUBF/Schnuerch and Erdfelder - 2020 - Controlling decision errors with minimal costs Th.pdf}
}

@article{schonbrodt2017,
  title = {Sequential {{Hypothesis Testing With Bayes Factors}}: Efficiently {{Testing Mean Differences}}},
  author = {Sch{\"o}nbrodt, Felix D and Zehetleitner, Michael and Wagenmakers, Eric-Jan and Perugini, Marco},
  year = {2017},
  journal = {Psychological Methods},
  volume = {22},
  number = {2},
  pages = {322--339},
  doi = {10.1037/met0000061},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/GIUWUUV8/Sch√∂nbrodt et al. - Sequential Hypothesis Testing With Bayes Factors .pdf}
}

@article{schonbrodt2018,
  title = {Bayes Factor Design Analysis: Planning for Compelling Evidence},
  author = {Sch{\"o}nbrodt, Felix D and Wagenmakers, Eric-Jan},
  year = {2018},
  journal = {Psychon Bull Rev},
  pages = {15},
  abstract = {A sizeable literature exists on the use of frequentist power analysis in the null-hypothesis significance testing (NHST) paradigm to facilitate the design of informative experiments. In contrast, there is almost no literature that discusses the design of experiments when Bayes factors (BFs) are used as a measure of evidence. Here we explore Bayes Factor Design Analysis (BFDA) as a useful tool to design studies for maximum efficiency and informativeness. We elaborate on three possible BF designs, (a) a fixed-n design, (b) an open-ended Sequential Bayes Factor (SBF) design, where researchers can test after each participant and can stop data collection whenever there is strong evidence for either H1 or H0, and (c) a modified SBF design that defines a maximal sample size where data collection is stopped regardless of the current state of evidence. We demonstrate how the properties of each design (i.e., expected strength of evidence, expected sample size, expected probability of misleading evidence, expected probability of weak evidence) can be evaluated using Monte Carlo simulations and equip researchers with the necessary information to compute their own Bayesian design analyses.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/CR8XPKXL/Sch√∂nbrodt - 2018 - Bayes factor design analysis Planning for compell.pdf}
}

@article{schweinsberg2021,
  title = {Same Data, Different Conclusions: Radical Dispersion in Empirical Results When Independent Analysts Operationalize and Test the Same Hypothesis},
  shorttitle = {Same Data, Different Conclusions},
  author = {Schweinsberg, Martin and Feldman, Michael and Staub, Nicola and {van den Akker}, Olmo R. and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and Liu, Yang and Althoff, Tim and Heer, Jeffrey and Kale, Alex and Mohamed, Zainab and Amireh, Hashem and Venkatesh Prasad, Vaishali and Bernstein, Abraham and Robinson, Emily and Snellman, Kaisa and Amy Sommer, S. and Otner, Sarah M. G. and Robinson, David and Madan, Nikhil and Silberzahn, Raphael and Goldstein, Pavel and Tierney, Warren and Murase, Toshio and Mandl, Benjamin and Viganola, Domenico and Strobl, Carolin and Schaumans, Catherine B. C. and Kelchtermans, Stijn and Naseeb, Chan and Mason Garrison, S. and Yarkoni, Tal and Richard Chan, C. S. and Adie, Prestone and Alaburda, Paulius and Albers, Casper and Alspaugh, Sara and Alstott, Jeff and Nelson, Andrew A. and {Ari{\~n}o de la Rubia}, Eduardo and Arzi, Adbi and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Baik, Jason and Winther Balling, Laura and Banker, Sachin and AA Baranger, David and Barr, Dale J. and {Barros-Rivera}, Brenda and Bauer, Matt and Blaise, Enuh and Boelen, Lisa and Bohle Carbonell, Katerina and Briers, Robert A. and Burkhard, Oliver and Canela, Miguel-Angel and Castrillo, Laura and Catlett, Timothy and Chen, Olivia and Clark, Michael and Cohn, Brent and Coppock, Alex and {Cuguer{\'o}-Escofet}, Nat{\`a}lia and Curran, Paul G. and {Cyrus-Lai}, Wilson and Dai, David and Valentino Dalla Riva, Giulio and Danielsson, Henrik and Russo, Rosaria de F. S. M. and {de Silva}, Niko and Derungs, Curdin and Dondelinger, Frank and {Duarte de Souza}, Carolina and Tyson Dube, B. and Dubova, Marina and Mark Dunn, Ben and Adriaan Edelsbrunner, Peter and Finley, Sara and Fox, Nick and Gnambs, Timo and Gong, Yuanyuan and Grand, Erin and Greenawalt, Brandon and Han, Dan and Hanel, Paul H. P. and Hong, Antony B. and Hood, David and Hsueh, Justin and Huang, Lilian and Hui, Kent N. and Hultman, Keith A. and Javaid, Azka and Ji Jiang, Lily and Jong, Jonathan and Kamdar, Jash and Kane, David and Kappler, Gregor and Kaszubowski, Erikson and Kavanagh, Christopher M. and {et al.}},
  year = {2021},
  journal = {Organizational Behavior and Human Decision Processes},
  volume = {165},
  pages = {228--249},
  publisher = {{Elsevier Science}},
  address = {{Netherlands}},
  issn = {1095-9920},
  doi = {10.1016/j.obhdp.2021.02.003},
  abstract = {In this crowdsourced initiative, independent analysts used the same dataset to test two hypotheses regarding the effects of scientists' gender and professional status on verbosity during group meetings. Not only the analytic approach but also the operationalizations of key variables were left unconstrained and up to individual analysts. For instance, analysts could choose to operationalize status as job title, institutional ranking, citation counts, or some combination. To maximize transparency regarding the process by which analytic choices are made, the analysts used a platform we developed called DataExplained to justify both preferred and rejected analytic paths in real time. Analyses lacking sufficient detail, reproducible code, or with statistical errors were excluded, resulting in 29 analyses in the final sample. Researchers reported radically different analyses and dispersed empirical outcomes, in a number of cases obtaining significant effects in opposite directions for the same research question. A Boba multiverse analysis demonstrates that decisions about how to operationalize variables explain variability in outcomes above and beyond statistical choices (e.g., covariates). Subjective researcher decisions play a critical role in driving the reported empirical results, underscoring the need for open data, systematic robustness checks, and transparency regarding both analytic paths taken and not taken. Implications for organizations and leaders, whose decision making relies in part on scientific findings, consulting reports, and internal analyses by data scientists, are discussed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {Crowdsourcing,Decision Making,Error Analysis,Freedom,Hypothesis Testing,Statistical Analysis},
  file = {/Users/shilaan/Zotero/storage/MMHJI5NM/Schweinsberg et al. - 2021 - Same data, different conclusions Radical dispersi.pdf;/Users/shilaan/Zotero/storage/M3W5PNII/2021-59292-001.html}
}

@article{silberzahn2015,
  title = {Crowdsourced Research: Many Hands Make Tight Work},
  shorttitle = {Crowdsourced Research},
  author = {Silberzahn, R. and Uhlmann, Eric L.},
  year = {2015},
  month = oct,
  journal = {Nature},
  volume = {526},
  number = {7572},
  pages = {189--191},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/526189a},
  abstract = {Crowdsourcing research can balance discussions, validate findings and better inform policy, say Raphael Silberzahn and Eric L. Uhlmann.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/T6QNHMHU/Silberzahn and Uhlmann - 2015 - Crowdsourced research Many hands make tight work.pdf;/Users/shilaan/Zotero/storage/ZD8DFACS/526189a.html}
}

@article{silberzahn2018,
  title = {Many {{Analysts}}, {{One Data Set}}: Making {{Transparent How Variations}} in {{Analytic Choices Affect Results}}},
  shorttitle = {Many {{Analysts}}, {{One Data Set}}},
  author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahn{\'i}k, {\v S}. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and {Gamez-Djokic}, M. and Glenz, A. and {Gordon-McKeon}, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and H{\"o}gden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schl{\"u}ter, E. and Sch{\"o}nbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Sp{\"o}rlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {337--356},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245917747646},
  abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts' prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
  langid = {english},
  keywords = {crowdsourcing science,data analysis,open data,open materials,scientific transparency},
  file = {/Users/shilaan/Zotero/storage/ZZXUF8NJ/Silberzahn et al. - 2018 - Many Analysts, One Data Set Making Transparent Ho.pdf}
}

@article{simmons2011,
  title = {False-{{Positive Psychology}}: Undisclosed {{Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/ACEEKR6Y/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{simmons2011a,
  title = {False-{{Positive Psychology}}: Undisclosed {{Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists? nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  file = {/Users/shilaan/Zotero/storage/AL6M8D2K/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{simmons2021,
  title = {Pre-Registration Is a {{Game Changer}}. {{But}}, {{Like Random Assignment}}, It Is {{Neither Necessary Nor Sufficient}} for {{Credible Science}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2021},
  journal = {Journal of Consumer Psychology},
  volume = {31},
  number = {1},
  pages = {177--180},
  issn = {1532-7663},
  doi = {10.1002/jcpy.1207},
  abstract = {We identify 15 claims Pham and Oh (2020) make to argue against pre-registration. We agree with 7 of the claims, but think that none of them justify delaying the encouragement and adoption of pre-registration. Moreover, while the claim they make in their title is correct\textemdash pre-registration is neither necessary nor sufficient for a credible science\textemdash this is also true of many our science's most valuable tools, such as random assignment. Indeed, both random assignment and pre-registration lead to more credible research. Pre-registration is a game changer.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1207},
  file = {/Users/shilaan/Zotero/storage/XY7AUUD5/Simmons et al. - 2021 - Pre-registration is a Game Changer. But, Like Rand.pdf;/Users/shilaan/Zotero/storage/FPTVA8LW/jcpy.html}
}

@article{steegen2016,
  title = {Increasing {{Transparency Through}} a {{Multiverse Analysis}}},
  author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
  year = {2016},
  month = sep,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {5},
  pages = {702--712},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691616658637},
  abstract = {Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.},
  langid = {english},
  keywords = {arbitrary choices,data processing,good research practices,multiverse analysis,selective reporting,transparency},
  file = {/Users/shilaan/Zotero/storage/4E5KQ2G5/Steegen et al. - 2016 - Increasing Transparency Through a Multiverse Analy.pdf}
}

@techreport{stefan2020,
  type = {Preprint},
  title = {Efficiency in {{Sequential Testing}}: Comparing the {{Sequential Probability Ratio Test}} and the {{Sequential Bayes Factor Test}}},
  shorttitle = {Efficiency in {{Sequential Testing}}},
  author = {Stefan, Angelika and Sch{\"o}nbrodt, Felix D. and Evans, Nathan J. and Wagenmakers, Eric-Jan},
  year = {2020},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/ry4fw},
  abstract = {In a sequential hypothesis test, the analyst checks at multiple steps during data collection whether sufficient evidence has accrued to make a decision about the tested hypotheses. As soon as sufficient information has been obtained, data collection is terminated. Here, we compare two sequential hypothesis testing procedures that have recently been proposed for use in psychological research: the Sequential Probability Ratio Test (SPRT; Schnuerch \& Erdfelder, 2020) and the Sequential Bayes Factor Test (SBFT; Sch\"onbrodt et al., 2017). We show that although the two methods have been presented as distinct methodologies in the past, they share many similarities and can even be regarded as two instances of the same overarching hypothesis testing framework. We demonstrate that the two methods use the same mechanisms for evidence monitoring and error control, and that differences in efficiency between the methods depend on the exact specification of the statistical models involved. Given the close relationship between the SPRT and SBFT, we argue that the choice of the sequential testing method should be regarded as a continuous choice within a unified framework rather than a dichotomous choice between two methods. We present several considerations researchers can make to navigate the design decisions in the SPRT and SBFT.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/83H7HFWR/Stefan et al. - 2020 - Efficiency in Sequential Testing Comparing the Se.pdf}
}

@techreport{tiokhin2020,
  type = {Preprint},
  title = {Competition for Priority and the Cultural Evolution of Research Strategies},
  author = {Tiokhin, Leonid and Yan, Minhua and Morgan, Tom},
  year = {2020},
  month = feb,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/x4t7q},
  abstract = {Incentives for priority of discovery are hypothesized to harm scientific reliability. Here, we evaluate this hypothesis by developing an evolutionary agent-based model of a competitive scientific process. We find that rewarding priority of discovery causes populations to culturally evolve towards conducting research with smaller samples. This reduces research reliability and the information-value of the average study. Increased startup costs for setting up single studies and increased payoffs for secondary results (a.k.a. ``scoop protection'') attenuate the negative effects of competition. Further, large rewards for negative results promote the evolution of smaller sample sizes. Our results confirm the logical coherence of ``scoop protection'' reforms at several journals. Our results also imply that reforms to increase scientific efficiency, such as rapid journal turnaround times, may produce collateral damage by incentivizing lower-quality research; in contrast, reforms that increase startup costs, such as preregistration and registered reports, generate incentives for higher-quality research.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/84FC4VKP/Tiokhin et al. - 2020 - Competition for priority and the cultural evolutio.pdf}
}

@book{tippett1931,
  title = {The Methods of Statistics; an Introduction Mainly for Workers in the Biological Sciences,},
  author = {Tippett, L. H. C},
  year = {1931},
  publisher = {{Williams \& Norgate Ltd.}},
  address = {{London}},
  langid = {english},
  annotation = {OCLC: 2029473}
}

@article{turner2017,
  title = {Research {{Design}} for {{Mixed Methods}}: A {{Triangulation}}-Based {{Framework}} and {{Roadmap}}},
  shorttitle = {Research {{Design}} for {{Mixed Methods}}},
  author = {Turner, Scott F. and Cardinal, Laura B. and Burton, Richard M.},
  year = {2017},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {20},
  number = {2},
  pages = {243--267},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428115610808},
  abstract = {All methods individually are flawed, but these limitations can be mitigated through mixed methods research, which combines methodologies to provide better answers to our research questions. In this study, we develop a research design framework for mixed methods work that is based on the principles of triangulation. Core elements for the research design framework include theoretical purpose, i.e., theory development and/or theory testing; and methodological purpose, i.e., prioritizing generalizability, precision in control and measurement, and authenticity of context. From this foundation, we consider how the multiple methodologies are linked together to accomplish the theoretical purpose, focusing on three types of linking processes: convergent triangulation, holistic triangulation, and convergent and holistic triangulation. We then consider the implications of these linking processes for the theory at hand, taking into account the following theoretical attributes: generality/specificity, simplicity/complexity, and accuracy/inaccuracy. Based on this research design framework, we develop a roadmap that can serve as a design guide for organizational scholars conducting mixed methods research studies.},
  file = {/Users/shilaan/Zotero/storage/ZKCTZLCR/Turner et al. - 2017 - Research Design for Mixed Methods A Triangulation.pdf}
}

@article{uhlmann2019,
  title = {Scientific {{Utopia III}}: Crowdsourcing {{Science}}},
  shorttitle = {Scientific {{Utopia III}}},
  author = {Uhlmann, Eric Luis and Ebersole, Charles R. and Chartier, Christopher R. and Errington, Timothy M. and Kidwell, Mallory C. and Lai, Calvin K. and McCarthy, Randy J. and Riegelman, Amy and Silberzahn, Raphael and Nosek, Brian A.},
  year = {2019},
  month = sep,
  journal = {Perspectives on Psychological Science},
  volume = {14},
  number = {5},
  pages = {711--733},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691619850561},
  abstract = {Most scientific research is conducted by small teams of investigators who together formulate hypotheses, collect data, conduct analyses, and report novel findings. These teams operate independently as vertically integrated silos. Here we argue that scientific research that is horizontally distributed can provide substantial complementary value, aiming to maximize available resources, promote inclusiveness and transparency, and increase rigor and reliability. This alternative approach enables researchers to tackle ambitious projects that would not be possible under the standard model. Crowdsourced scientific initiatives vary in the degree of communication between project members from largely independent work curated by a coordination team to crowd collaboration on shared activities. The potential benefits and challenges of large-scale collaboration span the entire research process: ideation, study design, data collection, data analysis, reporting, and peer review. Complementing traditional small science with crowdsourced approaches can accelerate the progress of science and improve the quality of scientific research.},
  file = {/Users/shilaan/Zotero/storage/TTY9XWX6/Uhlmann et al. - 2019 - Scientific Utopia III Crowdsourcing Science.pdf}
}

@article{vandoorn2020,
  title = {The {{JASP}} Guidelines for Conducting and Reporting a {{Bayesian}} Analysis},
  author = {Van Doorn, Johnny and {van den Bergh}, Don and B{\"o}hm, Udo and Dablander, Fabian and Derks, Koen and Draws, Tim and Etz, Alexander and Evans, Nathan J. and Gronau, Quentin F. and Haaf, Julia M. and Hinne, Max and Kucharsk{\'y}, {\v S}imon and Ly, Alexander and Marsman, Maarten and Matzke, Dora and Gupta, Akash R. Komarlu Narendra and Sarafoglou, Alexandra and Stefan, Angelika and Voelkel, Jan G. and Wagenmakers, Eric-Jan},
  year = {2020},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1531-5320},
  doi = {10.3758/s13423-020-01798-5},
  abstract = {Despite the increasing popularity of Bayesian inference in empirical research, few practical guidelines provide detailed recommendations for how to apply Bayesian procedures and interpret the results. Here we offer specific guidelines for four different stages of Bayesian statistical reasoning in a research setting: planning the analysis, executing the analysis, interpreting the results, and reporting the results. The guidelines for each stage are illustrated with a running example. Although the guidelines are geared towards analyses performed with the open-source statistical software JASP, most guidelines extend to Bayesian inference in general.},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/GGCS46C4/van Doorn et al. - 2020 - The JASP guidelines for conducting and reporting a.pdf}
}

@misc{wagenmakers,
  title = {A {{Compendium}} of {{Clean Graphs}} in {{R}}},
  author = {Wagenmakers, Eric-Jan and Gronau, Quentin F.},
  howpublished = {https://www.shinyapps.org/apps/RGraphCompendium/index.php\#prior-and-posterior},
  file = {/Users/shilaan/Zotero/storage/PLHAZAIR/index.html}
}

@article{wagenmakers2007,
  title = {A Practical Solution to the Pervasive Problems Ofp Values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03194105},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/54BTUF58/Wagenmakers - 2007 - A practical solution to the pervasive problems ofp.pdf}
}

@article{wagenmakers2018,
  title = {Bayesian Inference for Psychology. {{Part I}}: Theoretical Advantages and Practical Ramifications},
  shorttitle = {Bayesian Inference for Psychology. {{Part I}}},
  author = {Wagenmakers, Eric-Jan and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Love, Jonathon and Selker, Ravi and Gronau, Quentin F. and {\v S}m{\'i}ra, Martin and Epskamp, Sacha and Matzke, Dora and Rouder, Jeffrey N. and Morey, Richard D.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {35--57},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1343-3},
  abstract = {Bayesian parameter estimation and Bayesian hypothesis testing present attractive alternatives to classical inference using confidence intervals and p values. In part I of this series we outline ten prominent advantages of the Bayesian approach. Many of these advantages translate to concrete opportunities for pragmatic researchers. For instance, Bayesian hypothesis testing allows researchers to quantify evidence and monitor its progression as data come in, without needing to know the intention with which the data were collected. We end by countering several objections to Bayesian hypothesis testing. Part II of this series discusses JASP, a free and open source software program that makes it easy to conduct Bayesian estimation and testing for a range of popular statistical scenarios (Wagenmakers et al., this issue).},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/CVJH39RW/Wagenmakers et al. - 2018 - Bayesian inference for psychology. Part I Theoret.pdf}
}

@book{wald1973,
  title = {Sequential Analysis},
  author = {Wald, Abraham},
  year = {1973},
  publisher = {{Dover Publications}},
  address = {{New York}},
  isbn = {978-0-486-61579-0},
  langid = {english},
  lccn = {QA279.7 .W34 1973},
  keywords = {Sequential analysis},
  file = {/Users/shilaan/Zotero/storage/HY49WK9B/Wald - 1973 - Sequential analysis.pdf}
}

@article{wasserstein2019,
  title = {Moving to a {{World Beyond}} ``{\emph{p}} {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1583913},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/8IQ3HGSU/Wasserstein et al. - 2019 - Moving to a World Beyond ‚Äú p  0.05‚Äù.pdf}
}

@book{wassmer2016,
  title = {Group {{Sequential}} and {{Confirmatory Adaptive Designs}} in {{Clinical Trials}}},
  author = {Wassmer, Gernot and Brannath, Werner},
  year = {2016},
  series = {Springer {{Series}} in {{Pharmaceutical Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-32562-0},
  isbn = {978-3-319-32560-6 978-3-319-32562-0},
  langid = {english},
  file = {/Users/shilaan/Zotero/storage/B7BMC7DS/Wassmer and Brannath - 2016 - Group Sequential and Confirmatory Adaptive Designs.pdf}
}

@article{westberg1985,
  title = {Combining {{Independent Statistical Tests}}},
  author = {Westberg, Margareta},
  year = {1985},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {34},
  number = {3},
  pages = {287--296},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  doi = {10.2307/2987655},
  abstract = {In the present study two well-known combination methods, Fisher's and Tippett's, are compared according to their power. The calculations are made for normally and chi-square distributed test statistics. None of the two procedures is uniformly better than the other according to the power but sometimes the power curves cross each other. The calculated power-graphs give guidelines for when to use Fisher's method and when to use Tippett's.}
}

@book{whitley2000a,
  title = {The Intellectual and Social Organization of the Sciences},
  author = {Whitley, Richard},
  year = {2000},
  edition = {2nd ed},
  publisher = {{Oxford University Press}},
  address = {{Oxford ; New York}},
  isbn = {978-0-19-924053-1 978-0-19-924045-6},
  lccn = {Q175 .W65124 2000},
  keywords = {Philosophy,Science,Social aspects}
}

@misc{yarkoni2021,
  title = {The {{Generalizability Crisis}}},
  author = {Yarkoni, Tal},
  year = {2021},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/jqw35},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned\textemdash that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology\textemdash the linear mixed model\textemdash I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that whereas the "random effect" formalism is used pervasively in psychology to model inter-subject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that the failure to  problems many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  keywords = {generalization,inference,philosophy of science,prediction,psychology,Quantitative Methods,random effects,Social and Behavioral Sciences,statistics,Theory and Philosophy of Science},
  file = {/Users/shilaan/Zotero/storage/ZG5M2T4E/Yarkoni - 2019 - The Generalizability Crisis.pdf}
}


