---
title             : "Lay Perceptions of Scientific Findings: Swayed by the Crowd?"
shorttitle        : "Lay Perceptions of Scientific Findings"

author:
  - name          : "First author name"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    address       : ""
    email         : ""
  - name          : "Second author name"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Insert affiliation here"
    
abstract: |
  Crowd science is on the rise in behavioral science and aims to increase the credibility of scientific research. Whether it does, however, remains untested. We run an experiment to study whether crowd science improves lay perceptions of scientific findings. We focus on crowdsourced data analysis: a crowd of scientists independently analyzes the same dataset to estimate and report on a parameter of interest.  
  We compare the effects of providing lay consumers (*N* = 1,498) with a single parameter estimate (the single estimate condition) vs. multiple parameter estimates from a crowd that (a) vary slightly and are all positive, leading to the same qualitative conclusion (the "consistent crowd" condition) or (b) vary widely and are of both signs, leading to differing qualitative conclusions (the "inconsistent crowd" condition). In the single estimate condition, a single team of six researchers reports a 5% estimate; in the consistent crowd condition, six independent researchers report six low variance, high consensus estimates (2%, 4%, 5%, 5%, 6%, 8%; $M = 5\%, \sigma = 2\%$); in the inconsistent crowd condition, six independent researchers report six high variance, low consensus estimates (-6%, -2%, 5%, 5%, 12%, 16%; $M = 5\%, \sigma = 8.25\%$). We draw from theories on social norms and numerical cognition to hypothesize that consistent (inconsistent) crowd estimates will lead to higher (lower) posterior beliefs and credibility of the results, lower confidence in the precise average parameter estimate, and lower (higher) ratings of bias and error, than a single estimate. `r knitr::load_cache("results")`  
  In line with our preregistered hypotheses, we find that lay consumers of inconsistent (high variance, low consensus) crowd estimates have lower posterior beliefs, `r final_inconsistent`; find the results less credible, `r credibility_inconsistent`; have less confidence in the average effect size estimate, `r confidence_inconsistent`; and believe the results are more likely to stem from bias, `r bias_inconsistent`, and error, `r error_inconsistent`, than consumers of a single estimate. Contrary to our hypotheses, we do not find that consuming consistent (low variance, high consensus) crowd estimates improves lay perceptions of scientific findings: instead, to our surprise, it leads to lower posterior beliefs, `r final`, and higher ratings of error, `r error`, than a single estimate. In the future, it is important for crowd scientists to consider how -- e.g., by drawing from research on science communication -- to tackle science skepticism and effectively communicate variable crowd estimates to lay consumers.

  
keywords          : "Crowd science, Science skepticism, Variability, Numerical cognition"
wordcount         : "Abstract: 400, Project summary: 965"

bibliography      : ["references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
# devtools::install_github("crsh/papaja@devel")
library(papaja)
library(raincloudplots)
library(ggplot2)
library(broom)
library(purrr)
library(glue)
library(readr)
library(dplyr)
library(rmarkdown)
library(here)
library(icons)
library(rsvg)
```

Every day, important scientific findings are rejected at large. From man-made climate change to the safety and efficacy of Covid-19 vaccinations, science skepticism has run rampant among lay consumers in modern society [@hornsey2017]. To increase public faith in science, some have proposed the use of crowd science [@silberzahn2018; @uhlmann2019]. We explore the effects of scientific findings emerging from a crowd of researchers (vs. a typical research collaboration) on lay perceptions of scientific findings. In line with social norms theory [@miller2016], we expect that observing consensus among a crowd (the consistent crowd condition) will -- compared to the conclusion of a single scientist (the single estimate condition) -- increase conformity in opinion. Drawing from work on intuitive statistics [@gigerenzer2015a], we also expect laypeople to intuitively accord to the logic of "the wisdom of crowds": the ability of an aggregate of estimates (rather than a single estimate) to reduce noise stemming from individual bias or error [@schweinsberg2021].  
In contrast, when crowd estimates show low consensus and high variance (the inconsistent crowd condition), we predict that observers will be less swayed and more likely to attribute the findings to bias and error. In addition, due to the difficulty of lay reasoning about variation [@garfield2004], we predict an aversion to variability: i.e., we expect that observing variable estimates will decrease lay confidence in the precise average parameter estimate in both crowd conditions. In sum, our preregistered hypotheses are as follows: when laypeople observe multiple consistent (inconsistent) estimates from a crowd of independent scientists, we expect -- compared to a single estimate and controlling for prior beliefs -- higher (lower) posterior beliefs and credibility of the results, lower confidence in the precise average parameter estimate, and lower (greater) ratings of bias and error.  

# Methods
We report how we determined our sample size, all data exclusions, all manipulations, and all measures. We sampled UK- and US-based participants from the Prolific participant pool (native English speakers, over 18 years old). There was no prior work to base an excepted effect size on; due to our expectation of relatively small effect sizes and monetary constraints, we decided on an upper limit of 1,500 participants after exclusions. After participants report their prior beliefs about a research question ("Do religious people report higher well-being?"), we randomly allocate them to one of three conditions. In the single estimate condition, a single team of six researchers reports a 5% increase in well-being among religious people; in the consistent crowd condition, six independent researchers report six low variance, high consensus estimates (2%, 4%, 5%, 5%, 6%, 8%; $M = 5\%, \sigma = 2\%$); in the inconsistent crowd condition, six independent researchers report six high variance, low consensus estimates (-6%, -2%, 5%, 5%, 12%, 16%; $M = 5\%, \sigma = 8.25\%$). Afterwards, participants rate -- on a slider from 0% to 100% -- their posterior beliefs, the credibility of the results, their confidence in the aggregate 5% estimate, and the likelihood of researcher bias, error, and discretion (i.e., idiosyncratic choices). We did not preregister any hypotheses for the last measure; rated discretion is thus an exploratory measure.  

# Results

```{r results, include = FALSE, cache = TRUE}
df <- read.csv(here("data/data.csv"))
df <- df[3:nrow(df),]
n_recorded <- nrow(df)

df <- df %>% #take out participants who attempted to take the survey more than once
  filter(!PROLIFIC_PID %in% df$PROLIFIC_PID[duplicated(df$PROLIFIC_PID)])
n_more_than_once <- n_recorded - nrow(df)

df <- df %>% #take out participants who were screened out or did not consent
  filter(Consent == "I agree to participate in this research project")
n_screened_out <- n_recorded - n_more_than_once - nrow(df)

df <- df %>% #take out participants who failed or did not complete the attention check
  filter(Condition == Attention_check & Attention_check != "") 
n_failed_attention <- n_recorded - n_more_than_once - n_screened_out - nrow(df)
ns_bycondition <- table(df$Condition)

df <- df %>% 
  mutate(
    ID = 1:nrow(df),
    Condition = factor(Condition, #set reference category
                       levels = c("Single",           
                                  "Multi-consistent", 
                                  "Multi-inconsistent"))
  ) %>% 
  rename(
    Prior_beliefs = Prior.beliefs_1,
    Final_beliefs = Final.beliefs_1,
    Credibility = Credibility_1,
    Confidence = Confidence_1,
    Bias = Sources.Variability_1,
    Error = Sources.Variability_4,
    Discretion = Sources.Variability_5
  ) %>% 
  select(ID, Condition, Attention_check, Prior_beliefs:Discretion)

df[,4:ncol(df)] <- sapply(df[,4:ncol(df)], as.numeric) #recode variables to numeric

# Function to show inline results
results <- function(Predictor, Multi_Condition = "Consistent") {
  linear_mod <- lm(
    pull(df, {{ Predictor }}) ~ Condition + Prior_beliefs, data = df)
  
  apa_results <- apa_print(linear_mod)


  ifelse(Multi_Condition == "Consistent",
         paste(apa_results$estimate$ConditionMulti_consistent,
               strsplit(apa_results$statistic$ConditionMulti_consistent, ",")[[1]][2],
               sep = ", "), #print without t-values, otherwise use apa_results$full_result
         paste(apa_results$estimate$ConditionMulti_inconsistent,
               strsplit(apa_results$statistic$ConditionMulti_inconsistent, ",")[[1]][2],
               sep = ", ")
         )

}

# Cache results in objects for abstract
final <- results(Final_beliefs)
final_inconsistent <- results(Final_beliefs, "Inconsistent")
credibility <- results(Credibility)
credibility_inconsistent <- results(Credibility, "Inconsistent")
confidence <- results(Confidence)
confidence_inconsistent <- results(Confidence, "Inconsistent")
bias <- results(Bias)
bias_inconsistent <- results(Bias, "Inconsistent")
error <- results(Error)
error_inconsistent <- results(Error, "Inconsistent")
discretion <- results(Discretion)
discretion_inconsistent <- results(Discretion, "Inconsistent")

# Post-hoc t-test
df_multiconsistent <- df %>% 
  filter(Condition == "Multi-consistent")

t_test <- apa_print(t.test(df_multiconsistent$Final_beliefs, df_multiconsistent$Prior_beliefs, paired = TRUE))

t_apa <- paste(t_test$estimate, 
               strsplit(t_test$statistic, ",")[[1]][2], #take out t-value
               sep = ", ")
```

We recorded 2,019 responses; after excluding `r n_more_than_once` observations from participants who attempted to take the survey more than once, `r n_screened_out` participants who were screened out prior to starting the survey or did not consent, and `r n_failed_attention` participants who failed the attention check, our total sample size was 1,498 participants. As preregistered, we regressed all outcomes on prior beliefs and condition (with the single estimate condition as the reference category). Figure 1 displays the main findings. Compared to the single estimate condition, inconsistent crowd estimates decreased posterior beliefs (*b* = -22.80, *p* < .001, Cohen's *d* = -0.72), ratings of credibility (*b* = -6.54, *p* < .001, *d* = -0.21), and confidence in the parameter estimate (*b* = -9.09, *p* < .001, *d* = -0.26), and increased ratings of bias (*b* = 6.68, *p* < .001, *d* = 0.18), error (*b* = 5.89, *p* < .001, *d* = 0.18), and discretion (*b* = 9.22, *p* < .001, *d* = 0.29). Compared to the single estimate condition, consistent crowd estimates decreased posterior beliefs (*b* = -5.71, *p* < .001, *d* = -0.18) and increased ratings of error (*b* = 3.59, *p* = .016, *d* = 0.11) and discretion (*b* = 5.14, *p* < .001, *d* = 0.16); we found no significant effects on ratings of credibility (*b* = 1.24, *p* = .380, *d* = 0.04), confidence in the average parameter estimate (*b* = 0.49, *p* = .749, *d* = 0.01), or ratings of bias (*b* = 2.13, *p* = .192, *d* = 0.06). As hypothesized, inconsistent crowd estimates lead to lower posterior beliefs, credibility of the results, and confidence in the average parameter estimate, and higher ratings of bias and error, than a single estimate. Unexpectedly, consistent crowd estimates lead to lower posterior beliefs and higher ratings of error than a single estimate. However, it is worth noting that there is a positive difference between prior and posterior beliefs within the consistent crowd condition ($M_d$ = 4.75, *p* < .001, *d* = 0.19; see Figure 2). This finding clarifies that a consistent crowd may sway observers --- however, compared to a conventional, non-crowdsourced estimate, crowd estimates do not seem to improve lay perceptions of scientific findings.

# Discussion

Although crowd science aims to improve the credibility of scientific research, we find that lay consumers resist the variability and lack of consensus that often reflect its reality. To our surprise, even when results generated by a crowd are highly consistent, we find no improvement in lay perceptions. Further research is needed to consider the possible mechanisms behind this preliminary finding. In the future, it is important for crowd scientists to consider how to tackle science skepticism and effectively communicate variable estimates. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage

# Figures

## Figure 1
*Estimates of differences with the single estimate condition*
\noindent
```{r fig-1, echo=FALSE}
knitr::include_graphics("Figure1c.jpg", dpi = 1200)
```
*Note.* Figure 1 displays coefficient estimates (and 95% confidence intervals) of posterior beliefs, credibility, confidence, bias, error, and discretion in the two crowd conditions, compared to the single estimate condition (and controlling for prior beliefs). Green/red/gray error bars indicate positive/negative/insignificant findings, respectively.   

\newpage

## Figure 2

*Distribution of prior and posterior beliefs by condition*  
\noindent
```{r raincloud, echo=FALSE}
knitr::include_graphics("Figure2b.jpg", dpi = 1200)
```
*Note. * Prior beliefs are displayed in blue; posterior beliefs are displayed in orange. The respective boxes display the lower quartiles, medians, and upper quartiles of prior and posterior beliefs by condition.

<!-- Given the existing huge inefficiencies, substantial improvements are almost certainly feasible...but neither presence nor absence of revolutionary intent should be taken as a reliable surrogate for actual impact…  -->

<!-- Interventions to change the current system should not be accepted without proper scrutiny, even when they are reasonable and well-intended.” -->

<!-- John Ioannidis -->
